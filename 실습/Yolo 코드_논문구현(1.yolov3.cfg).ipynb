{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa6ccc2c",
   "metadata": {},
   "source": [
    "# Yolo 코드_논문구현\n",
    "***\n",
    "### 특징\n",
    "1. yolo에서 사용할 수 있는 input size는 총 3가지<br>\n",
    "  - 320 : 빠르지만 정확도 낮음<br>\n",
    "  - 416 : 중간<br>\n",
    "  - 609 : 느리지만 정확도 높음<br><br>\n",
    "\n",
    "출처 : https://deep-learning-study.tistory.com/568"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16055208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# 데이터 불러오기\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torchvision import utils\n",
    "from torchsummary import summary\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "# 그리기\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import albumentations.pytorch\n",
    "#from albumentations.pytorch import ToTensor\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d97698d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66835fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/JungHyeona/Documents/deeplearning_seminar/Biometry_20200722/train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91115b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path + \"img_info(x,y,w,h).csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0172c2a",
   "metadata": {},
   "source": [
    "## tranform 및 data불러오고 loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c3d05bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([\n",
    "        transforms.Resize((320, 320)),  # 이미지 크기 변경\n",
    "        transforms.ToTensor(),  # 이미지를 텐서로 변환\n",
    "        #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # 이미지 정규화\n",
    "    ])\n",
    "\n",
    "# 데이터셋 생성\n",
    "data_path = 'C:/Users/JungHyeona/Documents/deeplearning_seminar/Biometry_20200722/'\n",
    "train_datasets = datasets.ImageFolder(data_path + 'img', transform=data_transforms)\n",
    "val_datasets = datasets.ImageFolder(root='val/val_img', transform=data_transforms)\n",
    "\n",
    "# 데이터로더 생성\n",
    "train_loaders = torch.utils.data.DataLoader(train_datasets, batch_size=20, shuffle=True)\n",
    "val_loaders = torch.utils.data.DataLoader(val_datasets, batch_size=20, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05cf16e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class label: 0, Folder name: AC\n",
      "Class label: 1, Folder name: FL\n",
      "Class label: 2, Folder name: HC\n",
      "Class label: 3, Folder name: HUM\n"
     ]
    }
   ],
   "source": [
    "# 클래스 라벨과 폴더 이름 출력\n",
    "\n",
    "classes = train_datasets.classes\n",
    "\n",
    "for label, name in enumerate(classes):\n",
    "    print(f\"Class label: {label}, Folder name: {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcefdc0",
   "metadata": {},
   "source": [
    "## transform한 이미지 확인해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "794368eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규화된 x,y,w,h를 이미지 크기에 맞게 변경\n",
    "def rescale_bbox(bb, W, H):\n",
    "    x,y,w,h = bb\n",
    "    return [x*W, y*H, w*W, h*H]\n",
    "\n",
    "# 바운딩 박스 색상\n",
    "COLORS = np.random.randint(0, 255, size=(80,3),dtype='uint8')\n",
    "\n",
    "# image 출력 함수 정의\n",
    "def show_img_bbox(img, targets, classes=classes):\n",
    "    if torch.is_tensor(img):\n",
    "        img=to_pil_image(img)\n",
    "    if torch.is_tensor(targets):\n",
    "        targets=targets.numpy()[:,1:]\n",
    "    \n",
    "    W, H = img.size\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    for tg in targets:\n",
    "        id_=int(tg[4])\n",
    "        bbox=tg[:4]\n",
    "        bbox=rescale_bbox(bbox,W,H)\n",
    "        xc,yc,w,h = bbox\n",
    "\n",
    "        color = [int(c) for c in COLORS[id_]]\n",
    "        name=classes[id_]\n",
    "\n",
    "        draw.rectangle(((xc-w/2, yc-h/2), (xc+w/2, yc+h/2)), outline=tuple(color), width=3)\n",
    "        draw.text((xc-w/2, yc-h/2), name, fill=(255,255,255,0))\n",
    "    plt.imshow(np.array(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73ef3356",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (27283712.py, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\JungHyeona\\AppData\\Local\\Temp\\ipykernel_6680\\27283712.py\"\u001b[1;36m, line \u001b[1;32m12\u001b[0m\n\u001b[1;33m    img =\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# transforms가 적용된 sample image 확인\n",
    "np.random.seed(2)\n",
    "\n",
    "grid_size = 2\n",
    "rnd_ind = np.random.randint(0, len(train_datasets), grid_size)\n",
    "print('image indices:',rnd_ind)\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "# i=[0, 1] / indice=[23720, 6637]\n",
    "for i, indice in enumerate(rnd_ind):\n",
    "    img = \n",
    "    label, _ = train_ds[indice]\n",
    "    plt.subplot(1, grid_size, i+1)\n",
    "    show_img_bbox(img, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218113aa",
   "metadata": {},
   "source": [
    "# Yolo 모델 구축\n",
    "***\n",
    "1. configuration file 분석으로 구현하기<br>\n",
    "2. 직접 모델 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b0c950",
   "metadata": {},
   "source": [
    "### 1. yolov3.cfg 파일 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eefc94e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "con_path = path + 'yolo/darknet-master/cfg/yolov3.cfg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d0baef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config 파일을 분석하는 함수를 정의합니다.\n",
    "def parse_model_config(con_path):\n",
    "    # cfg 파일 열기\n",
    "    cfg_file = open(con_path, 'r')\n",
    "    # 문자열 데이터 읽어오기 \n",
    "    lines = cfg_file.read().split('\\n') #['[net]', '# Testing', '# batch=1', '....' ]\n",
    "\n",
    "    # 데이터 전처리\n",
    "    # startswith('#'): 문자열이 # 로 시작하는지 여부를 알려줍니다. \n",
    "    lines = [x for x in lines if x and not x.startswith('#')] # ['[net]', 'batch=64', '...']\n",
    "    # 공백 제거\n",
    "    lines = [x.rstrip().lstrip() for x in lines]\n",
    "\n",
    "    blocks_list = []\n",
    "    for line in lines:\n",
    "        if line.startswith('['): # [net]\n",
    "            blocks_list.append({}) # {}\n",
    "            blocks_list[-1]['type'] = line[1:-1].rstrip() # [{'type': 'net'}]\n",
    "        else:\n",
    "            key, value = line.split('=') # batch=64 -> batch, 64\n",
    "            value = value.strip() # 공백 제거\n",
    "            blocks_list[-1][key.rstrip()] = value.strip() # 'batch':'64'\n",
    "\n",
    "    return blocks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78b0a714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg 파일 분석\n",
    "blocks_list = parse_model_config(con_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c091eb83",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'net',\n",
       "  'batch': '1',\n",
       "  'subdivisions': '1',\n",
       "  'width': '416',\n",
       "  'height': '416',\n",
       "  'channels': '3',\n",
       "  'momentum': '0.9',\n",
       "  'decay': '0.0005',\n",
       "  'angle': '0',\n",
       "  'saturation': '1.5',\n",
       "  'exposure': '1.5',\n",
       "  'hue': '.1',\n",
       "  'learning_rate': '0.001',\n",
       "  'burn_in': '1000',\n",
       "  'max_batches': '500200',\n",
       "  'policy': 'steps',\n",
       "  'steps': '400000,450000',\n",
       "  'scales': '.1,.1'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '32',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '64',\n",
       "  'size': '3',\n",
       "  'stride': '2',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '32',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '64',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': '-3', 'activation': 'linear'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '128',\n",
       "  'size': '3',\n",
       "  'stride': '2',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '64',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '128',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': '-3', 'activation': 'linear'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '64',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '128',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': '-3', 'activation': 'linear'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'size': '3',\n",
       "  'stride': '2',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '128',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': '-3', 'activation': 'linear'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '128',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': '-3', 'activation': 'linear'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '128',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': '-3', 'activation': 'linear'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '128',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': '-3', 'activation': 'linear'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '128',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': '-3', 'activation': 'linear'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '128',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': '-3', 'activation': 'linear'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '128',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': '-3', 'activation': 'linear'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '128',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': '-3', 'activation': 'linear'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '512',\n",
       "  'size': '3',\n",
       "  'stride': '2',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '512',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': '-3', 'activation': 'linear'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '512',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': '-3', 'activation': 'linear'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '512',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': '-3', 'activation': 'linear'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '512',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': '-3', 'activation': 'linear'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '512',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': '-3', 'activation': 'linear'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '512',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': '-3', 'activation': 'linear'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '512',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': '-3', 'activation': 'linear'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '512',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': '-3', 'activation': 'linear'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '1024',\n",
       "  'size': '3',\n",
       "  'stride': '2',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '512',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '1024',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': '-3', 'activation': 'linear'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '512',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '1024',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': '-3', 'activation': 'linear'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '512',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '1024',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': '-3', 'activation': 'linear'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '512',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '1024',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': '-3', 'activation': 'linear'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '512',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'filters': '1024',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '512',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'filters': '1024',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '512',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'filters': '1024',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'filters': '255',\n",
       "  'activation': 'linear'},\n",
       " {'type': 'yolo',\n",
       "  'mask': '6,7,8',\n",
       "  'anchors': '10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326',\n",
       "  'classes': '80',\n",
       "  'num': '9',\n",
       "  'jitter': '.3',\n",
       "  'ignore_thresh': '.7',\n",
       "  'truth_thresh': '1',\n",
       "  'random': '1'},\n",
       " {'type': 'route', 'layers': '-4'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'upsample', 'stride': '2'},\n",
       " {'type': 'route', 'layers': '-1, 61'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'filters': '512',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'filters': '512',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'filters': '512',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'filters': '255',\n",
       "  'activation': 'linear'},\n",
       " {'type': 'yolo',\n",
       "  'mask': '3,4,5',\n",
       "  'anchors': '10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326',\n",
       "  'classes': '80',\n",
       "  'num': '9',\n",
       "  'jitter': '.3',\n",
       "  'ignore_thresh': '.7',\n",
       "  'truth_thresh': '1',\n",
       "  'random': '1'},\n",
       " {'type': 'route', 'layers': '-4'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '128',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'upsample', 'stride': '2'},\n",
       " {'type': 'route', 'layers': '-1, 36'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '128',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'filters': '256',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '128',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'filters': '256',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '128',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'filters': '256',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'filters': '255',\n",
       "  'activation': 'linear'},\n",
       " {'type': 'yolo',\n",
       "  'mask': '0,1,2',\n",
       "  'anchors': '10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326',\n",
       "  'classes': '80',\n",
       "  'num': '9',\n",
       "  'jitter': '.3',\n",
       "  'ignore_thresh': '.7',\n",
       "  'truth_thresh': '1',\n",
       "  'random': '1'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blocks_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61aba563",
   "metadata": {},
   "source": [
    "### 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b828914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EmptyLayer를 정의합니다.\n",
    "# EmptyLayer는 residual unit의 shortcut과 FPN의 lateral connection 용도로 사용합니다.\n",
    "class EmptyLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e5ec3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLOLayer를 정의합니다.\n",
    "# YOLOLayer는 13x13, 26x26, 52x52 피쳐맵에서 예측을 수행합니다.\n",
    "class YOLOLayer(nn.Module):\n",
    "    def __init__(self, anchors, num_classes, img_dim=320):\n",
    "        super().__init__()\n",
    "        self.anchors = anchors # three anchor per YOLO layer\n",
    "        self.num_anchors = len(anchors) # 3\n",
    "        self.num_classes = num_classes\n",
    "        self.img_dim = img_dim\n",
    "        self.grid_size = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: batch_size, channels, H, W\n",
    "        batch_size = x.size(0)\n",
    "        grid_size = x.size(2) # S = 13 or 26 or 52\n",
    "        device = x.device\n",
    "\n",
    "        prediction = x.view(batch_size, self.num_anchors, self.num_classes + 5, \n",
    "                            grid_size, grid_size) # shape = (batch, 3, 25, S, S)\n",
    "\n",
    "        # (batch, 3, 25, S, S) -> (batch, 3, S, S, 25)\n",
    "        prediction = prediction.permute(0, 1, 3, 4, 2)\n",
    "        prediction = prediction.contiguous()\n",
    "\n",
    "        obj_score = torch.sigmoid(prediction[..., 4]) # 클래스\n",
    "        pred_cls = torch.sigmoid(prediction[..., 5:]) # 바운딩 박스 좌표\n",
    "\n",
    "        if grid_size != self.grid_size:\n",
    "            # grid_size 갱신, cell index 생성, anchor 정규화\n",
    "            self.compute_grid_offsets(grid_size, cuda=x.is_cuda)\n",
    "\n",
    "        # bounding box prediction\n",
    "        pred_boxes = self.transform_outputs(prediction)\n",
    "\n",
    "        # batch, num_anchor x S x S, 25\n",
    "        # ex) at 13x13 -> [batch, 507, 25], at 26x26 -> [batch, 2028, 85], at 52x52 -> [batch, 10647, 85]\n",
    "        # 최종적으로 YOLO는 10647개의 바운딩박스를 예측합니다.\n",
    "        output = torch.cat((pred_boxes.view(batch_size, -1, 4),\n",
    "                            obj_score.view(batch_size, -1, 1),\n",
    "                            pred_cls.view(batch_size, -1, self.num_classes)), -1)\n",
    "        return output\n",
    "\n",
    "\n",
    "    def compute_grid_offsets(self, grid_size, cuda=True):\n",
    "        self.grid_size = grid_size # ex) 13, 26, 52\n",
    "        self.stride = self.img_dim / self.grid_size # ex) 32, 16, 8\n",
    "\n",
    "        # cell index 생성\n",
    "        # 1, 1, S, 1\n",
    "        self.grid_x = torch.arange(grid_size, device=device).repeat(1, 1, grid_size, 1).type(torch.float32)\n",
    "        # 1, 1, 1, S\n",
    "        self.grid_y = torch.arange(grid_size, device=device).repeat(1, 1, grid_size, 1).transpose(3, 2).type(torch.float32)\n",
    "\n",
    "        # anchors를 feature map 크기로 정규화, [0~1] 범위\n",
    "        # ex) (10, 13), (16, 30), (33, 23) / stride\n",
    "        scaled_anchors = [(a_w / self.stride, a_h / self.stride) for a_w, a_h in self.anchors]\n",
    "        # tensor로 변환\n",
    "        self.scaled_anchors = torch.tensor(scaled_anchors, device=device)\n",
    "\n",
    "        # shape=(3,2) -> (1,1,3,1)\n",
    "        self.anchor_w = self.scaled_anchors[:, 0:1].view((1, self.num_anchors, 1, 1))\n",
    "        # shape=(3,2) -> (1,1,3,1)\n",
    "        self.anchor_h = self.scaled_anchors[:, 1:2].view((1, self.num_anchors, 1, 1))\n",
    "\n",
    "\n",
    "    def transform_outputs(self, prediction):\n",
    "        # pridiction (batch, 3, S, S, 25)\n",
    "        device = prediction.device\n",
    "        x = torch.sigmoid(prediction[..., 0]) # sigmoid(box x), 예측값을 sigmoid로 감싸서 [0~1] 범위\n",
    "        y = torch.sigmoid(prediction[..., 1]) # sigmoid(box y), 예측값을 sigmoid로 감싸서 [0~1] 범위\n",
    "        w = prediction[..., 2] # 예측한 바운딩 박스 너비\n",
    "        h = prediction[..., 3] # 예측한 바운딩 박스 높이\n",
    "\n",
    "        pred_boxes = torch.zeros_like(prediction[..., :4])#.to(device)\n",
    "        pred_boxes[..., 0] = x.data + self.grid_x # sigmoid(box x) + cell x 좌표\n",
    "        pred_boxes[..., 1] = y.data + self.grid_y # sigmoid(box y) + cell y 좌표\n",
    "        pred_boxes[..., 2] = torch.exp(w.data) * self.anchor_w\n",
    "        pred_boxes[..., 3] = torch.exp(h.data) * self.anchor_h\n",
    "\n",
    "        return pred_boxes * self.stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d9a60a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer를 생성하는 함수를 정의합니다.\n",
    "def create_layers(blocks_list):\n",
    "    hyperparams = blocks_list[0]\n",
    "    channels_list = [int(hyperparams['channels'])]\n",
    "    module_list = nn.ModuleList()\n",
    "\n",
    "    for layer_ind, layer_dict in enumerate(blocks_list[1:]):\n",
    "        modules = nn.Sequential()\n",
    "\n",
    "        if layer_dict['type'] == 'convolutional':\n",
    "            filters = int(layer_dict['filters'])\n",
    "            kernel_size = int(layer_dict['size'])\n",
    "            pad = (kernel_size - 1) // 2\n",
    "            bn = layer_dict.get('batch_normalize', 0)\n",
    "\n",
    "            conv2d = nn.Conv2d(in_channels=channels_list[-1], out_channels=filters, kernel_size=kernel_size,\n",
    "                               stride=int(layer_dict['stride']), padding=pad, bias=not bn)\n",
    "            modules.add_module('conv_{0}'.format(layer_ind), conv2d)\n",
    "\n",
    "            if bn:\n",
    "                bn_layer = nn.BatchNorm2d(filters, momentum=0.9, eps=1e-5)\n",
    "                modules.add_module('batch_norm_{0}'.format(layer_ind), bn_layer)\n",
    "            \n",
    "            if layer_dict['activation'] == 'leaky':\n",
    "                activn = nn.LeakyReLU(0.1)\n",
    "                modules.add_module('leky_{0}'.format(layer_ind), activn)\n",
    "\n",
    "        elif layer_dict[\"type\"] == \"upsample\":\n",
    "            stride = int(layer_dict[\"stride\"])\n",
    "            upsample = nn.Upsample(scale_factor = stride)\n",
    "            modules.add_module(\"upsample_{}\".format(layer_ind), upsample) \n",
    "\n",
    "        elif layer_dict[\"type\"] == \"shortcut\":\n",
    "            backwards=int(layer_dict[\"from\"])\n",
    "            filters = channels_list[1:][backwards]\n",
    "            modules.add_module(\"shortcut_{}\".format(layer_ind), EmptyLayer())\n",
    "            \n",
    "        elif layer_dict[\"type\"] == \"route\":\n",
    "            layers = [int(x) for x in layer_dict[\"layers\"].split(\",\")]\n",
    "            filters = sum([channels_list[1:][l] for l in layers])\n",
    "            modules.add_module(\"route_{}\".format(layer_ind), EmptyLayer())\n",
    "\n",
    "        elif layer_dict[\"type\"] == \"yolo\":\n",
    "            anchors = [int(a) for a in layer_dict[\"anchors\"].split(\",\")]\n",
    "            anchors = [(anchors[i], anchors[i + 1]) for i in range(0, len(anchors), 2)]\n",
    "\n",
    "            # ex) at 13x13, 'mask': '6,7,8'\n",
    "            # mask는 anchors index를 의미합니다.\n",
    "            # yolo layer당 3개의 anchors를 할당 합니다.\n",
    "            # mask는 yolo layer feature map size에 알맞는 anchors를 설정합니다.\n",
    "            mask = [int(m) for m in layer_dict[\"mask\"].split(\",\")]\n",
    "            \n",
    "            anchors = [anchors[i] for i in mask] # 3 anchors\n",
    "            \n",
    "            num_classes = int(layer_dict[\"classes\"]) # 20\n",
    "            img_size = int(hyperparams[\"height\"]) # 320 \n",
    "            \n",
    "            yolo_layer = YOLOLayer(anchors, num_classes, img_size)\n",
    "            modules.add_module(\"yolo_{}\".format(layer_ind), yolo_layer)\n",
    "            \n",
    "        module_list.append(modules)       \n",
    "        channels_list.append(filters)\n",
    "\n",
    "    return hyperparams, module_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "15d843a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# module_list로 Darknet 정의\n",
    "class Darknet(nn.Module):\n",
    "    def __init__(self, config_path, img_size=320):\n",
    "        super(Darknet, self).__init__()\n",
    "        self.blocks_list = parse_model_config(config_path)\n",
    "        self.hyperparams, self.module_list = create_layers(self.blocks_list)\n",
    "        self.img_size = img_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        img_dim = x.shape[2]\n",
    "        layer_outputs, yolo_outputs = [], []\n",
    "        \n",
    "        # blocks_list: config 파일 분석한 결과\n",
    "        # module_list: blocks_list로 생성한 module\n",
    "        for block, module in zip(self.blocks_list[1:], self.module_list):\n",
    "            if block[\"type\"] in [\"convolutional\", \"upsample\", \"maxpool\"]:\n",
    "                x = module(x)        \n",
    "                \n",
    "                \n",
    "            elif block[\"type\"] == \"shortcut\":\n",
    "                layer_ind = int(block[\"from\"]) # -3\n",
    "                x = layer_outputs[-1] + layer_outputs[layer_ind] # shortcut connection\n",
    "\n",
    "            #  {'type': 'yolo', 'mask': '3,4,5', 'anchors': '10,13, ...}\n",
    "            elif block[\"type\"] == \"yolo\":\n",
    "                x= module[0](x) # get yolo layer output\n",
    "                yolo_outputs.append(x)\n",
    "            elif block[\"type\"] == \"route\": #  {'type': 'route', 'layers': '-1, 61'}\n",
    "                x = torch.cat([layer_outputs[int(l_i)] \n",
    "                               for l_i in block[\"layers\"].split(\",\")], 1)\n",
    "            layer_outputs.append(x)\n",
    "        yolo_out_cat = torch.cat(yolo_outputs, 1) # 3개의 output을 하나로 연결\n",
    "        return yolo_out_cat, yolo_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7b1613",
   "metadata": {},
   "source": [
    "### 모델 생성 및 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5cb167ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24200\\2576115088.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0myolo_out_cat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myolo_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myolo_out_cat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myolo_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0myolo_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0myolo_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24200\\2721175779.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[1;31m#  {'type': 'yolo', 'mask': '3,4,5', 'anchors': '10,13, ...}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"type\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"yolo\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m                 \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# get yolo layer output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m                 \u001b[0myolo_outputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"type\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"route\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#  {'type': 'route', 'layers': '-1, 61'}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24200\\2160701619.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;31m# bounding box prediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mpred_boxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform_outputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;31m# batch, num_anchor x S x S, 25\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24200\\2160701619.py\u001b[0m in \u001b[0;36mtransform_outputs\u001b[1;34m(self, prediction)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[0mpred_boxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#.to(device)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m         \u001b[0mpred_boxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrid_x\u001b[0m \u001b[1;31m# sigmoid(box x) + cell x 좌표\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[0mpred_boxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrid_y\u001b[0m \u001b[1;31m# sigmoid(box y) + cell y 좌표\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[0mpred_boxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manchor_w\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "model = Darknet(con_path)#.to(device)\n",
    "x = torch.rand(1,3,320,320)#.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    yolo_out_cat, yolo_outputs = model.forward(x)\n",
    "    print(yolo_out_cat.shape)\n",
    "    print(yolo_outputs[0].shape,yolo_outputs[1].shape,yolo_outputs[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c29ac45",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 320, 320]             864\n",
      "       BatchNorm2d-2         [-1, 32, 320, 320]              64\n",
      "         LeakyReLU-3         [-1, 32, 320, 320]               0\n",
      "            Conv2d-4         [-1, 64, 160, 160]          18,432\n",
      "       BatchNorm2d-5         [-1, 64, 160, 160]             128\n",
      "         LeakyReLU-6         [-1, 64, 160, 160]               0\n",
      "            Conv2d-7         [-1, 32, 160, 160]           2,048\n",
      "       BatchNorm2d-8         [-1, 32, 160, 160]              64\n",
      "         LeakyReLU-9         [-1, 32, 160, 160]               0\n",
      "           Conv2d-10         [-1, 64, 160, 160]          18,432\n",
      "      BatchNorm2d-11         [-1, 64, 160, 160]             128\n",
      "        LeakyReLU-12         [-1, 64, 160, 160]               0\n",
      "           Conv2d-13          [-1, 128, 80, 80]          73,728\n",
      "      BatchNorm2d-14          [-1, 128, 80, 80]             256\n",
      "        LeakyReLU-15          [-1, 128, 80, 80]               0\n",
      "           Conv2d-16           [-1, 64, 80, 80]           8,192\n",
      "      BatchNorm2d-17           [-1, 64, 80, 80]             128\n",
      "        LeakyReLU-18           [-1, 64, 80, 80]               0\n",
      "           Conv2d-19          [-1, 128, 80, 80]          73,728\n",
      "      BatchNorm2d-20          [-1, 128, 80, 80]             256\n",
      "        LeakyReLU-21          [-1, 128, 80, 80]               0\n",
      "           Conv2d-22           [-1, 64, 80, 80]           8,192\n",
      "      BatchNorm2d-23           [-1, 64, 80, 80]             128\n",
      "        LeakyReLU-24           [-1, 64, 80, 80]               0\n",
      "           Conv2d-25          [-1, 128, 80, 80]          73,728\n",
      "      BatchNorm2d-26          [-1, 128, 80, 80]             256\n",
      "        LeakyReLU-27          [-1, 128, 80, 80]               0\n",
      "           Conv2d-28          [-1, 256, 40, 40]         294,912\n",
      "      BatchNorm2d-29          [-1, 256, 40, 40]             512\n",
      "        LeakyReLU-30          [-1, 256, 40, 40]               0\n",
      "           Conv2d-31          [-1, 128, 40, 40]          32,768\n",
      "      BatchNorm2d-32          [-1, 128, 40, 40]             256\n",
      "        LeakyReLU-33          [-1, 128, 40, 40]               0\n",
      "           Conv2d-34          [-1, 256, 40, 40]         294,912\n",
      "      BatchNorm2d-35          [-1, 256, 40, 40]             512\n",
      "        LeakyReLU-36          [-1, 256, 40, 40]               0\n",
      "           Conv2d-37          [-1, 128, 40, 40]          32,768\n",
      "      BatchNorm2d-38          [-1, 128, 40, 40]             256\n",
      "        LeakyReLU-39          [-1, 128, 40, 40]               0\n",
      "           Conv2d-40          [-1, 256, 40, 40]         294,912\n",
      "      BatchNorm2d-41          [-1, 256, 40, 40]             512\n",
      "        LeakyReLU-42          [-1, 256, 40, 40]               0\n",
      "           Conv2d-43          [-1, 128, 40, 40]          32,768\n",
      "      BatchNorm2d-44          [-1, 128, 40, 40]             256\n",
      "        LeakyReLU-45          [-1, 128, 40, 40]               0\n",
      "           Conv2d-46          [-1, 256, 40, 40]         294,912\n",
      "      BatchNorm2d-47          [-1, 256, 40, 40]             512\n",
      "        LeakyReLU-48          [-1, 256, 40, 40]               0\n",
      "           Conv2d-49          [-1, 128, 40, 40]          32,768\n",
      "      BatchNorm2d-50          [-1, 128, 40, 40]             256\n",
      "        LeakyReLU-51          [-1, 128, 40, 40]               0\n",
      "           Conv2d-52          [-1, 256, 40, 40]         294,912\n",
      "      BatchNorm2d-53          [-1, 256, 40, 40]             512\n",
      "        LeakyReLU-54          [-1, 256, 40, 40]               0\n",
      "           Conv2d-55          [-1, 128, 40, 40]          32,768\n",
      "      BatchNorm2d-56          [-1, 128, 40, 40]             256\n",
      "        LeakyReLU-57          [-1, 128, 40, 40]               0\n",
      "           Conv2d-58          [-1, 256, 40, 40]         294,912\n",
      "      BatchNorm2d-59          [-1, 256, 40, 40]             512\n",
      "        LeakyReLU-60          [-1, 256, 40, 40]               0\n",
      "           Conv2d-61          [-1, 128, 40, 40]          32,768\n",
      "      BatchNorm2d-62          [-1, 128, 40, 40]             256\n",
      "        LeakyReLU-63          [-1, 128, 40, 40]               0\n",
      "           Conv2d-64          [-1, 256, 40, 40]         294,912\n",
      "      BatchNorm2d-65          [-1, 256, 40, 40]             512\n",
      "        LeakyReLU-66          [-1, 256, 40, 40]               0\n",
      "           Conv2d-67          [-1, 128, 40, 40]          32,768\n",
      "      BatchNorm2d-68          [-1, 128, 40, 40]             256\n",
      "        LeakyReLU-69          [-1, 128, 40, 40]               0\n",
      "           Conv2d-70          [-1, 256, 40, 40]         294,912\n",
      "      BatchNorm2d-71          [-1, 256, 40, 40]             512\n",
      "        LeakyReLU-72          [-1, 256, 40, 40]               0\n",
      "           Conv2d-73          [-1, 128, 40, 40]          32,768\n",
      "      BatchNorm2d-74          [-1, 128, 40, 40]             256\n",
      "        LeakyReLU-75          [-1, 128, 40, 40]               0\n",
      "           Conv2d-76          [-1, 256, 40, 40]         294,912\n",
      "      BatchNorm2d-77          [-1, 256, 40, 40]             512\n",
      "        LeakyReLU-78          [-1, 256, 40, 40]               0\n",
      "           Conv2d-79          [-1, 512, 20, 20]       1,179,648\n",
      "      BatchNorm2d-80          [-1, 512, 20, 20]           1,024\n",
      "        LeakyReLU-81          [-1, 512, 20, 20]               0\n",
      "           Conv2d-82          [-1, 256, 20, 20]         131,072\n",
      "      BatchNorm2d-83          [-1, 256, 20, 20]             512\n",
      "        LeakyReLU-84          [-1, 256, 20, 20]               0\n",
      "           Conv2d-85          [-1, 512, 20, 20]       1,179,648\n",
      "      BatchNorm2d-86          [-1, 512, 20, 20]           1,024\n",
      "        LeakyReLU-87          [-1, 512, 20, 20]               0\n",
      "           Conv2d-88          [-1, 256, 20, 20]         131,072\n",
      "      BatchNorm2d-89          [-1, 256, 20, 20]             512\n",
      "        LeakyReLU-90          [-1, 256, 20, 20]               0\n",
      "           Conv2d-91          [-1, 512, 20, 20]       1,179,648\n",
      "      BatchNorm2d-92          [-1, 512, 20, 20]           1,024\n",
      "        LeakyReLU-93          [-1, 512, 20, 20]               0\n",
      "           Conv2d-94          [-1, 256, 20, 20]         131,072\n",
      "      BatchNorm2d-95          [-1, 256, 20, 20]             512\n",
      "        LeakyReLU-96          [-1, 256, 20, 20]               0\n",
      "           Conv2d-97          [-1, 512, 20, 20]       1,179,648\n",
      "      BatchNorm2d-98          [-1, 512, 20, 20]           1,024\n",
      "        LeakyReLU-99          [-1, 512, 20, 20]               0\n",
      "          Conv2d-100          [-1, 256, 20, 20]         131,072\n",
      "     BatchNorm2d-101          [-1, 256, 20, 20]             512\n",
      "       LeakyReLU-102          [-1, 256, 20, 20]               0\n",
      "          Conv2d-103          [-1, 512, 20, 20]       1,179,648\n",
      "     BatchNorm2d-104          [-1, 512, 20, 20]           1,024\n",
      "       LeakyReLU-105          [-1, 512, 20, 20]               0\n",
      "          Conv2d-106          [-1, 256, 20, 20]         131,072\n",
      "     BatchNorm2d-107          [-1, 256, 20, 20]             512\n",
      "       LeakyReLU-108          [-1, 256, 20, 20]               0\n",
      "          Conv2d-109          [-1, 512, 20, 20]       1,179,648\n",
      "     BatchNorm2d-110          [-1, 512, 20, 20]           1,024\n",
      "       LeakyReLU-111          [-1, 512, 20, 20]               0\n",
      "          Conv2d-112          [-1, 256, 20, 20]         131,072\n",
      "     BatchNorm2d-113          [-1, 256, 20, 20]             512\n",
      "       LeakyReLU-114          [-1, 256, 20, 20]               0\n",
      "          Conv2d-115          [-1, 512, 20, 20]       1,179,648\n",
      "     BatchNorm2d-116          [-1, 512, 20, 20]           1,024\n",
      "       LeakyReLU-117          [-1, 512, 20, 20]               0\n",
      "          Conv2d-118          [-1, 256, 20, 20]         131,072\n",
      "     BatchNorm2d-119          [-1, 256, 20, 20]             512\n",
      "       LeakyReLU-120          [-1, 256, 20, 20]               0\n",
      "          Conv2d-121          [-1, 512, 20, 20]       1,179,648\n",
      "     BatchNorm2d-122          [-1, 512, 20, 20]           1,024\n",
      "       LeakyReLU-123          [-1, 512, 20, 20]               0\n",
      "          Conv2d-124          [-1, 256, 20, 20]         131,072\n",
      "     BatchNorm2d-125          [-1, 256, 20, 20]             512\n",
      "       LeakyReLU-126          [-1, 256, 20, 20]               0\n",
      "          Conv2d-127          [-1, 512, 20, 20]       1,179,648\n",
      "     BatchNorm2d-128          [-1, 512, 20, 20]           1,024\n",
      "       LeakyReLU-129          [-1, 512, 20, 20]               0\n",
      "          Conv2d-130         [-1, 1024, 10, 10]       4,718,592\n",
      "     BatchNorm2d-131         [-1, 1024, 10, 10]           2,048\n",
      "       LeakyReLU-132         [-1, 1024, 10, 10]               0\n",
      "          Conv2d-133          [-1, 512, 10, 10]         524,288\n",
      "     BatchNorm2d-134          [-1, 512, 10, 10]           1,024\n",
      "       LeakyReLU-135          [-1, 512, 10, 10]               0\n",
      "          Conv2d-136         [-1, 1024, 10, 10]       4,718,592\n",
      "     BatchNorm2d-137         [-1, 1024, 10, 10]           2,048\n",
      "       LeakyReLU-138         [-1, 1024, 10, 10]               0\n",
      "          Conv2d-139          [-1, 512, 10, 10]         524,288\n",
      "     BatchNorm2d-140          [-1, 512, 10, 10]           1,024\n",
      "       LeakyReLU-141          [-1, 512, 10, 10]               0\n",
      "          Conv2d-142         [-1, 1024, 10, 10]       4,718,592\n",
      "     BatchNorm2d-143         [-1, 1024, 10, 10]           2,048\n",
      "       LeakyReLU-144         [-1, 1024, 10, 10]               0\n",
      "          Conv2d-145          [-1, 512, 10, 10]         524,288\n",
      "     BatchNorm2d-146          [-1, 512, 10, 10]           1,024\n",
      "       LeakyReLU-147          [-1, 512, 10, 10]               0\n",
      "          Conv2d-148         [-1, 1024, 10, 10]       4,718,592\n",
      "     BatchNorm2d-149         [-1, 1024, 10, 10]           2,048\n",
      "       LeakyReLU-150         [-1, 1024, 10, 10]               0\n",
      "          Conv2d-151          [-1, 512, 10, 10]         524,288\n",
      "     BatchNorm2d-152          [-1, 512, 10, 10]           1,024\n",
      "       LeakyReLU-153          [-1, 512, 10, 10]               0\n",
      "          Conv2d-154         [-1, 1024, 10, 10]       4,718,592\n",
      "     BatchNorm2d-155         [-1, 1024, 10, 10]           2,048\n",
      "       LeakyReLU-156         [-1, 1024, 10, 10]               0\n",
      "          Conv2d-157          [-1, 512, 10, 10]         524,288\n",
      "     BatchNorm2d-158          [-1, 512, 10, 10]           1,024\n",
      "       LeakyReLU-159          [-1, 512, 10, 10]               0\n",
      "          Conv2d-160         [-1, 1024, 10, 10]       4,718,592\n",
      "     BatchNorm2d-161         [-1, 1024, 10, 10]           2,048\n",
      "       LeakyReLU-162         [-1, 1024, 10, 10]               0\n",
      "          Conv2d-163          [-1, 512, 10, 10]         524,288\n",
      "     BatchNorm2d-164          [-1, 512, 10, 10]           1,024\n",
      "       LeakyReLU-165          [-1, 512, 10, 10]               0\n",
      "          Conv2d-166         [-1, 1024, 10, 10]       4,718,592\n",
      "     BatchNorm2d-167         [-1, 1024, 10, 10]           2,048\n",
      "       LeakyReLU-168         [-1, 1024, 10, 10]               0\n",
      "          Conv2d-169          [-1, 512, 10, 10]         524,288\n",
      "     BatchNorm2d-170          [-1, 512, 10, 10]           1,024\n",
      "       LeakyReLU-171          [-1, 512, 10, 10]               0\n",
      "          Conv2d-172         [-1, 1024, 10, 10]       4,718,592\n",
      "     BatchNorm2d-173         [-1, 1024, 10, 10]           2,048\n",
      "       LeakyReLU-174         [-1, 1024, 10, 10]               0\n",
      "          Conv2d-175          [-1, 255, 10, 10]         261,375\n",
      "       YOLOLayer-176              [-1, 300, 85]               0\n",
      "          Conv2d-177          [-1, 256, 10, 10]         131,072\n",
      "     BatchNorm2d-178          [-1, 256, 10, 10]             512\n",
      "       LeakyReLU-179          [-1, 256, 10, 10]               0\n",
      "        Upsample-180          [-1, 256, 20, 20]               0\n",
      "          Conv2d-181          [-1, 256, 20, 20]         196,608\n",
      "     BatchNorm2d-182          [-1, 256, 20, 20]             512\n",
      "       LeakyReLU-183          [-1, 256, 20, 20]               0\n",
      "          Conv2d-184          [-1, 512, 20, 20]       1,179,648\n",
      "     BatchNorm2d-185          [-1, 512, 20, 20]           1,024\n",
      "       LeakyReLU-186          [-1, 512, 20, 20]               0\n",
      "          Conv2d-187          [-1, 256, 20, 20]         131,072\n",
      "     BatchNorm2d-188          [-1, 256, 20, 20]             512\n",
      "       LeakyReLU-189          [-1, 256, 20, 20]               0\n",
      "          Conv2d-190          [-1, 512, 20, 20]       1,179,648\n",
      "     BatchNorm2d-191          [-1, 512, 20, 20]           1,024\n",
      "       LeakyReLU-192          [-1, 512, 20, 20]               0\n",
      "          Conv2d-193          [-1, 256, 20, 20]         131,072\n",
      "     BatchNorm2d-194          [-1, 256, 20, 20]             512\n",
      "       LeakyReLU-195          [-1, 256, 20, 20]               0\n",
      "          Conv2d-196          [-1, 512, 20, 20]       1,179,648\n",
      "     BatchNorm2d-197          [-1, 512, 20, 20]           1,024\n",
      "       LeakyReLU-198          [-1, 512, 20, 20]               0\n",
      "          Conv2d-199          [-1, 255, 20, 20]         130,815\n",
      "       YOLOLayer-200             [-1, 1200, 85]               0\n",
      "          Conv2d-201          [-1, 128, 20, 20]          32,768\n",
      "     BatchNorm2d-202          [-1, 128, 20, 20]             256\n",
      "       LeakyReLU-203          [-1, 128, 20, 20]               0\n",
      "        Upsample-204          [-1, 128, 40, 40]               0\n",
      "          Conv2d-205          [-1, 128, 40, 40]          49,152\n",
      "     BatchNorm2d-206          [-1, 128, 40, 40]             256\n",
      "       LeakyReLU-207          [-1, 128, 40, 40]               0\n",
      "          Conv2d-208          [-1, 256, 40, 40]         294,912\n",
      "     BatchNorm2d-209          [-1, 256, 40, 40]             512\n",
      "       LeakyReLU-210          [-1, 256, 40, 40]               0\n",
      "          Conv2d-211          [-1, 128, 40, 40]          32,768\n",
      "     BatchNorm2d-212          [-1, 128, 40, 40]             256\n",
      "       LeakyReLU-213          [-1, 128, 40, 40]               0\n",
      "          Conv2d-214          [-1, 256, 40, 40]         294,912\n",
      "     BatchNorm2d-215          [-1, 256, 40, 40]             512\n",
      "       LeakyReLU-216          [-1, 256, 40, 40]               0\n",
      "          Conv2d-217          [-1, 128, 40, 40]          32,768\n",
      "     BatchNorm2d-218          [-1, 128, 40, 40]             256\n",
      "       LeakyReLU-219          [-1, 128, 40, 40]               0\n",
      "          Conv2d-220          [-1, 256, 40, 40]         294,912\n",
      "     BatchNorm2d-221          [-1, 256, 40, 40]             512\n",
      "       LeakyReLU-222          [-1, 256, 40, 40]               0\n",
      "          Conv2d-223          [-1, 255, 40, 40]          65,535\n",
      "       YOLOLayer-224             [-1, 4800, 85]               0\n",
      "================================================================\n",
      "Total params: 61,949,149\n",
      "Trainable params: 61,949,149\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.17\n",
      "Forward/backward pass size (MB): 529.07\n",
      "Params size (MB): 236.32\n",
      "Estimated Total Size (MB): 766.56\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (3, 320, 320))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25537ee3",
   "metadata": {},
   "source": [
    "## loss function 정의\n",
    "***\n",
    "출처 : https://github.com/PacktPublishing/PyTorch-Computer-Vision-Cookbook/blob/master/Chapter05/Chapter%205.ipynb\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1172c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_batch(output,targets, params_loss, opt=None):\n",
    "    ignore_thres=params_loss[\"ignore_thres\"]\n",
    "    scaled_anchors= params_loss[\"scaled_anchors\"] # 정규화된 anchor   \n",
    "    mse_loss= params_loss[\"mse_loss\"] # nn.MSELoss\n",
    "    bce_loss= params_loss[\"bce_loss\"] # nn.BCELoss, 이진 분류에서 사용\n",
    "    \n",
    "    num_yolos=params_loss[\"num_yolos\"] # 3\n",
    "    num_anchors= params_loss[\"num_anchors\"] # 3\n",
    "    obj_scale= params_loss[\"obj_scale\"] # 1\n",
    "    noobj_scale= params_loss[\"noobj_scale\"] # 100\n",
    "\n",
    "    loss = 0.0\n",
    "\n",
    "    for yolo_ind in range(num_yolos):\n",
    "        yolo_out = output[yolo_ind] # yolo_out: batch, num_boxes, class+coordinates\n",
    "        batch_size, num_bbxs, _ = yolo_out.shape\n",
    "\n",
    "        # get grid size\n",
    "        gz_2 = num_bbxs/num_anchors # ex) at 13x13, 507 / 3\n",
    "        grid_size=int(np.sqrt(gz_2))\n",
    "\n",
    "        # (batch, num_boxes, class+coordinates) -> (batch, num_anchors, S, S, class+coordinates)\n",
    "        yolo_out = yolo_out.view(batch_size, num_anchors, grid_size, grid_size, -1)\n",
    "\n",
    "        pred_boxes = yolo_out[:,:,:,:,:4] # get box coordinates\n",
    "        x,y,w,h = transform_bbox(pred_boxes, scaled_anchors[yolo_ind]) # cell 내에서 x,y 좌표와  \n",
    "        pred_conf = yolo_out[:,:,:,:,4] # get confidence\n",
    "        pred_cls_prob = yolo_out[:,:,:,:,5:]\n",
    "\n",
    "        yolo_targets = get_yolo_targets({\n",
    "            'pred_cls_prob':pred_cls_prob,\n",
    "            'pred_boxes':pred_boxes,\n",
    "            'targets':targets,\n",
    "            'anchors':scaled_anchors[yolo_ind],\n",
    "            'ignore_thres':ignore_thres,\n",
    "        })\n",
    "\n",
    "        obj_mask=yolo_targets[\"obj_mask\"]        \n",
    "        noobj_mask=yolo_targets[\"noobj_mask\"]            \n",
    "        tx=yolo_targets[\"tx\"]                \n",
    "        ty=yolo_targets[\"ty\"]                    \n",
    "        tw=yolo_targets[\"tw\"]                        \n",
    "        th=yolo_targets[\"th\"]                            \n",
    "        tcls=yolo_targets[\"tcls\"]                                \n",
    "        t_conf=yolo_targets[\"t_conf\"]\n",
    "\n",
    "        loss_x = mse_loss(x[obj_mask], tx[obj_mask])\n",
    "        loss_y = mse_loss(y[obj_mask], ty[obj_mask])\n",
    "        loss_w = mse_loss(w[obj_mask], tw[obj_mask])\n",
    "        loss_h = mse_loss(h[obj_mask], th[obj_mask])\n",
    "        \n",
    "        loss_conf_obj = bce_loss(pred_conf[obj_mask], t_conf[obj_mask])\n",
    "        loss_conf_noobj = bce_loss(pred_conf[noobj_mask], t_conf[noobj_mask])\n",
    "        loss_conf = obj_scale * loss_conf_obj + noobj_scale * loss_conf_noobj\n",
    "        loss_cls = bce_loss(pred_cls_prob[obj_mask], tcls[obj_mask])\n",
    "        loss += loss_x + loss_y + loss_w + loss_h + loss_conf + loss_cls\n",
    "        \n",
    "    if opt is not None:\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417477e3",
   "metadata": {},
   "source": [
    "### transform_bbox는 전체 이미지 x, y좌표에서 sell내의 x,y좌표로 변경. w, h는 anchor box에 맞게 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4baeb036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_bbox(bbox, anchors):\n",
    "    # bbox: predicted bbox coordinates\n",
    "    # anchors: scaled anchors\n",
    "\n",
    "    x = bbox[:,:,:,:,0]\n",
    "    y = bbox[:,:,:,:,1]\n",
    "    w = bbox[:,:,:,:,2]\n",
    "    h = bbox[:,:,:,:,3]\n",
    "    anchor_w = anchors[:,0].view((1,3,1,1))\n",
    "    anchor_h = anchors[:,1].view((1,3,1,1))\n",
    "\n",
    "    x = x-x.floor() # 전체 이미지의 x 좌표에서 셀 내의 x좌표로 변경\n",
    "    y = y-y.floor() # 전체 이미지의 y 좌표에서 셀 내의 y좌표로 변경\n",
    "    w = torch.log(w / anchor_w + 1e-16)\n",
    "    h = torch.log(h / anchor_h + 1e-16)\n",
    "    return x, y, w, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5de72775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yolo_targets(params):\n",
    "    pred_boxes = params['pred_boxes']\n",
    "    pred_cls_prob = params['pred_cls_prob']\n",
    "    target = params['targets'] # batchsize, cls, cx, cy, w, h\n",
    "    anchors = params['anchors']\n",
    "    ignore_thres = params['ignore_thres']\n",
    "\n",
    "    batch_size = pred_boxes.size(0)\n",
    "    num_anchors = pred_boxes.size(1)\n",
    "    grid_size = pred_boxes.size(2)\n",
    "    num_cls = pred_cls_prob.size(-1)\n",
    "\n",
    "\n",
    "    sizeT = batch_size, num_anchors, grid_size, grid_size\n",
    "    obj_mask = torch.zeros(sizeT, device=device, dtype=torch.uint8)\n",
    "    noobj_mask = torch.ones(sizeT, device=device, dtype=torch.uint8)\n",
    "    tx = torch.zeros(sizeT, device=device, dtype=torch.float32)\n",
    "    ty = torch.zeros(sizeT, device=device, dtype=torch.float32)\n",
    "    tw = torch.zeros(sizeT, device=device, dtype=torch.float32)\n",
    "    th = torch.zeros(sizeT, device=device, dtype=torch.float32)\n",
    "\n",
    "    sizeT = batch_size, num_anchors, grid_size, grid_size, num_cls\n",
    "    tcls = torch.zeros(sizeT, device=device, dtype=torch.float32)\n",
    "\n",
    "    # target = batch, cx, cy, w, h, class\n",
    "    target_bboxes = target[:, 1:5] * grid_size\n",
    "    t_xy = target_bboxes[:, :2]\n",
    "    t_wh = target_bboxes[:, 2:]\n",
    "    t_x, t_y = t_xy.t() # .t(): 전치\n",
    "    t_w, t_h = t_wh.t() # .t(): 전치\n",
    "\n",
    "    grid_i, grid_j = t_xy.long().t() # .long(): int로 변환\n",
    "\n",
    "    # anchor와 target의 iou 계산\n",
    "    iou_with_anchors = [get_iou_WH(anchor, t_wh) for anchor in anchors]\n",
    "    iou_with_anchors = torch.stack(iou_with_anchors)\n",
    "    best_iou_wa, best_anchor_ind = iou_with_anchors.max(0) # iou가 가장 높은 anchor 추출\n",
    "\n",
    "    batch_inds, target_labels = target[:, 0].long(), target[:, 5].long()\n",
    "    obj_mask[batch_inds, best_anchor_ind, grid_j, grid_i] = 1 # iou가 가장 높은 anchor 할당\n",
    "    noobj_mask[batch_inds, best_anchor_ind, grid_j, grid_i] = 0\n",
    "\n",
    "    # threshold 보다 높은 iou를 지닌 anchor\n",
    "    # iou가 가장 높은 anchor만 할당하면 되기 때문입니다.\n",
    "    for ind, iou_wa in enumerate(iou_with_anchors.t()):\n",
    "        noobj_mask[batch_inds[ind], iou_wa > ignore_thres, grid_j[ind], grid_i[ind]] = 0\n",
    "\n",
    "    # cell 내에서 x,y로 변환\n",
    "    tx[batch_inds, best_anchor_ind, grid_j, grid_i] = t_x - t_x.float()\n",
    "    ty[batch_inds, best_anchor_ind, grid_j, grid_i] = t_y - t_y.float()\n",
    "\n",
    "    anchor_w = anchors[best_anchor_ind][:, 0]\n",
    "    tw[batch_inds, best_anchor_ind, grid_j, grid_i] = torch.log(t_w / anchor_w + 1e-16)\n",
    "\n",
    "    anchor_h = anchors[best_anchor_ind][:, 1]\n",
    "    th[batch_inds, best_anchor_ind, grid_j, grid_i] = torch.log(t_h / anchor_h + 1e-16)\n",
    "\n",
    "    tcls[batch_inds, best_anchor_ind, grid_j, grid_i, target_labels] = 1\n",
    "\n",
    "    output = {\n",
    "        'obj_mask': obj_mask,\n",
    "        'noobj_mask': noobj_mask,\n",
    "        'tx': tx,\n",
    "        'ty': ty,\n",
    "        'tw': tw,\n",
    "        'th': th,\n",
    "        'tcls': tcls,\n",
    "        't_conf': obj_mask.float(),\n",
    "    }\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfe1dba",
   "metadata": {},
   "source": [
    "### anchor와 target box의 IoU 계산 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de76474f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iou_WH(wh1, wh2):\n",
    "    wh2 = wh2.t()\n",
    "    w1, h1 = wh1[0], wh1[1]\n",
    "    w2, h2 = wh2[0], wh2[1]\n",
    "    inter_area = torch.min(w1, w2) * torch.min(h1, h2)\n",
    "    union_area = (w1 * h1 + 1e-16) + w2 * h2 - inter_area\n",
    "    return inter_area / union_area"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db76bf0",
   "metadata": {},
   "source": [
    "# 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4da36fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.Adam(model.parameters(), lr=1e-3)\n",
    "lr_scheduler = ReduceLROnPlateau(opt, mode='min',factor=0.5, patience=20,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7cf78cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 현재 lr 계산하는 함수\n",
    "def get_lr(opt):\n",
    "    for param_group in opt.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab8b82e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch당 loss 계산하는 함수\n",
    "def loss_epoch(model, params_loss, dataset_dl, sanity_check=False, opt=None):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    len_data = len(dataset_dl.dataset)\n",
    "    running_metrics = {}\n",
    "    \n",
    "    for xb, yb in dataset_dl:\n",
    "        yb = yb.to(device)\n",
    "        _, output = model((xb).to(device))\n",
    "        loss_b = get_loss_batch(output,yb, params_loss,opt)\n",
    "        running_loss += loss_b\n",
    "        \n",
    "        if sanity_check is True:\n",
    "            break \n",
    "    loss = running_loss / float(len_data)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d178d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val(model, params):\n",
    "    num_epochs = params[\"num_epochs\"]\n",
    "    params_loss = params[\"params_loss\"]\n",
    "    opt = params[\"optimizer\"]\n",
    "    train_dl = params[\"train_dl\"]\n",
    "    val_dl = params[\"val_dl\"]\n",
    "    sanity_check = params[\"sanity_check\"]\n",
    "    lr_scheduler = params[\"lr_scheduler\"]\n",
    "    path2weights = params[\"path2weights\"]\n",
    "    \n",
    "    \n",
    "    loss_history = {\n",
    "        \"train\": [],\n",
    "        \"val\": [],\n",
    "    }\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = float('inf') \n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        current_lr = get_lr(opt)\n",
    "        print('Epoch {}/{}, current lr={}'.format(epoch, num_epochs - 1, current_lr)) \n",
    "        model.train()\n",
    "        train_loss = loss_epoch(model, params_loss, train_dl, sanity_check, opt)\n",
    "        loss_history[\"train\"].append(train_loss)  \n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = loss_epoch(model,params_loss,val_dl,sanity_check)\n",
    "        loss_history[\"val\"].append(val_loss)\n",
    "        \n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            torch.save(model.state_dict(), path2weights)\n",
    "            print(\"Copied best model weights!\")\n",
    "            print('Get best val loss')\n",
    "            \n",
    "        lr_scheduler.step(val_loss)\n",
    "        if current_lr != get_lr(opt):\n",
    "            print(\"Loading best model weights!\")\n",
    "            model.load_state_dict(best_model_wts) \n",
    "        print(\"train loss: %.6f, val loss: %.6f, time: %.4f min\" %(train_loss, val_loss, (time.time()-start_time)/60))\n",
    "        print(\"-\"*10) \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7dc0647e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path2models = \"./models/\"\n",
    "\n",
    "if not os.path.exists(path2models):\n",
    "        os.mkdir(path2models)\n",
    "        \n",
    "scaled_anchors =[\n",
    "    model.module_list[82][0].scaled_anchors,\n",
    "    model.module_list[94][0].scaled_anchors,\n",
    "    model.module_list[106][0].scaled_anchors\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f26d877d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_loss = nn.MSELoss(reduction=\"sum\")\n",
    "bce_loss = nn.BCELoss(reduction=\"sum\")\n",
    "params_loss ={\n",
    "    \"scaled_anchors\" : scaled_anchors,\n",
    "    \"ignore_thres\": 0.5,\n",
    "    \"mse_loss\": mse_loss,\n",
    "    \"bce_loss\": bce_loss,\n",
    "    \"num_yolos\": 3,\n",
    "    \"num_anchors\": 3,\n",
    "    \"obj_scale\": 1,\n",
    "    \"noobj_scale\": 100,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "11eaad5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_train ={\n",
    "    \"num_epochs\": 3,\n",
    "    \"optimizer\": opt,\n",
    "    \"params_loss\": params_loss,\n",
    "    \"train_dl\": train_loaders,\n",
    "    \"val_dl\": val_loaders,\n",
    "    \"sanity_check\": False,\n",
    "    \"lr_scheduler\": lr_scheduler,\n",
    "    \"path2weights\": path2models+\"weights.pt\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b84931a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/2, current lr=0.001\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 2.00 GiB total capacity; 1.73 GiB already allocated; 0 bytes free; 1.73 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24200\\3888270808.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_hist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_val\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24200\\281436681.py\u001b[0m in \u001b[0;36mtrain_val\u001b[1;34m(model, params)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Epoch {}/{}, current lr={}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_lr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msanity_check\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[0mloss_history\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"train\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24200\\983830352.py\u001b[0m in \u001b[0;36mloss_epoch\u001b[1;34m(model, params_loss, dataset_dl, sanity_check, opt)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataset_dl\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0myb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0myb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mloss_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_loss_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0myb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss_b\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 2.00 GiB total capacity; 1.73 GiB already allocated; 0 bytes free; 1.73 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "model, loss_hist = train_val(model, params_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d340c981",
   "metadata": {},
   "source": [
    "## loss progress 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b76a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = params_train['num_epochs']\n",
    "\n",
    "# Plot train-val loss\n",
    "plt.title('Train-Val Loss')\n",
    "plt.plot(range(1, num_epochs+1), loss_hist['train'], label='train')\n",
    "plt.plot(range(1, num_epochs+1), loss_hist['val'], label='val')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Training Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6581ee70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
