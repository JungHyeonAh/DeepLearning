{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ed85e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3b736db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as FT\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 이미지 불러오기 및 그리기\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c834e447",
   "metadata": {},
   "source": [
    "## 변수 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "67994407",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_dir = './'\n",
    "img_dir = ori_dir + 'image/'\n",
    "label_dir = ori_dir + 'label_txt/'\n",
    "\n",
    "train_csv = ori_dir + 'png_txt.csv'\n",
    "\n",
    "img_size = 416\n",
    "S = 7   # grid cell w,h크기\n",
    "B = 5\n",
    "C = 4\n",
    "\n",
    "classes = [ \"AC\", \"FL\", \"HC\", \"HUM\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5e2a0547",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "에러 내용 : TypeError: __call__() takes 2 positional arguments but 3 were given. self.\n",
    "https://stackoverflow.com/questions/62341052/typeerror-call-takes-2-positional-arguments-but-3-were-given-to-train-ra\n",
    "\n",
    "해결\n",
    "아래 있는 Compose 함수를 호출해서 transform을 하니 해결이 됨.\n",
    "'''\n",
    "class Compose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img, bboxes):\n",
    "        for t in self.transforms:\n",
    "            img, bboxes = t(img), bboxes\n",
    "\n",
    "        return img, bboxes\n",
    "\n",
    "transform = Compose([transforms.Resize((448, 448)), transforms.ToTensor(),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "35ea393b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 123\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Hyperparameters etc. \n",
    "DEVICE = \"cuda\" if torch.cuda.is_available else \"cpu\"   # torch.device('cpu')\n",
    "BATCH_SIZE = 4 # 64 in original paper but I don't have that much vram, grad accum?\n",
    "EPOCHS = 3\n",
    "NUM_WORKERS = 2\n",
    "PIN_MEMORY = True\n",
    "LOAD_MODEL = False\n",
    "LOAD_MODEL_FILE = \"overfit.pth.tar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1552db11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "048c47a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA GeForce GTX 750 Ti\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1325680f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03d1a6bf",
   "metadata": {},
   "source": [
    "# bbox가 이미지에서 잘 위치한지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4ac5141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef draw_bbox(img_file, boxes):\\n    # 이미지를 로드합니다.\\n    image = cv2.imread(img_file)\\n    \\n    # 바운딩 박스 좌표를 추출합니다.\\n    c, x, y, w, h = boxes\\n    x1, y1 = int((x - w/2) * 1024) , int((y - h/2) * 512)\\n    x2, y2 = int((x + w/2) * 1024) , int((y + h/2) * 512)\\n\\n    # 바운딩 박스를 시각화합니다.\\n    cv2.rectangle(image, (x1, y1), (x2, y2), (0, 0, 255), 2)\\n    # center 좌표 보기.\\n    #cv2.line(image, (x, y), (x, y), (0, 0, 255), 3)\\n    # 해당 이미지 class 확인\\n    cv2.putText(image, str(c), (x2, y1), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\\n    \\n    \\n    # 시각화된 이미지를 보여줍니다.\\n    cv2.imshow('Bounding Boxes', image)\\n    cv2.waitKey(0)\\n    cv2.destroyAllWindows()\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def draw_bbox(img_file, boxes):\n",
    "    # 이미지를 로드합니다.\n",
    "    image = cv2.imread(img_file)\n",
    "    \n",
    "    # 바운딩 박스 좌표를 추출합니다.\n",
    "    c, x, y, w, h = boxes\n",
    "    x1, y1 = int((x - w/2) * 1024) , int((y - h/2) * 512)\n",
    "    x2, y2 = int((x + w/2) * 1024) , int((y + h/2) * 512)\n",
    "\n",
    "    # 바운딩 박스를 시각화합니다.\n",
    "    cv2.rectangle(image, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "    # center 좌표 보기.\n",
    "    #cv2.line(image, (x, y), (x, y), (0, 0, 255), 3)\n",
    "    # 해당 이미지 class 확인\n",
    "    cv2.putText(image, str(c), (x2, y1), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
    "    \n",
    "    \n",
    "    # 시각화된 이미지를 보여줍니다.\n",
    "    cv2.imshow('Bounding Boxes', image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dfc028f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimg_file = img_dir + df['img_png'][0]\\nboxes = info['class'][0] , info['center_x'][0] , info['center_y'][0] , info['w'][0] , info['h'][0]\\n\\ndraw_bbox(img_file , boxes)\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "img_file = img_dir + df['img_png'][0]\n",
    "boxes = info['class'][0] , info['center_x'][0] , info['center_y'][0] , info['w'][0] , info['h'][0]\n",
    "\n",
    "draw_bbox(img_file , boxes)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cb6417",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29d56756",
   "metadata": {},
   "source": [
    "# Dataset 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "15b8ae75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "        self, csv_file, img_dir, label_dir, S=7, B=2, C=4, transform=None,\n",
    "    ):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 이미지 객체 정보 가져오기.\n",
    "        label_path = os.path.join(self.label_dir, self.annotations.iloc[index, 1])\n",
    "        boxes = []\n",
    "        \n",
    "        with open(label_path) as f:\n",
    "            for label in f.readlines():\n",
    "                # 이 순서대로 txt파일에 저장되어있음.\n",
    "                class_label, x, y, width, height = [\n",
    "                    float(x) if float(x) != int(float(x)) else int(x)\n",
    "                    for x in label.replace(\"\\n\", \"\").split()\n",
    "                ]\n",
    "                \n",
    "                # boxes 변수에 이미지 정보 한번에 배열로 저장하기.\n",
    "                boxes.append([class_label, x, y, width, height])\n",
    "                \n",
    "                \n",
    "        # 이미지 파일 불러오기.\n",
    "        img_path = os.path.join(self.img_dir, self.annotations.iloc[index, 0])\n",
    "        image = Image.open(img_path)\n",
    "        \n",
    "        # 이미지 정보를 tensor로 변환해주기.\n",
    "        boxes = torch.tensor(boxes)\n",
    "\n",
    "        if self.transform:\n",
    "            image, boxes = self.transform(image, boxes)\n",
    "\n",
    "        # ====================================================== \n",
    "        # ENCODING\n",
    "        # 이미지를 SxS그리드로 나누고, feature map의 tensor는 S x S x (B*5 + C). 5 : x, y, w, h, confidence\n",
    "        # label_matrix : ground truth box 중심좌표 계산 후 confidence score, bbox좌표 저장.\n",
    "        label_matrix = torch.zeros((self.S, self.S, self.C + 5 * self.B))\n",
    "        for box in boxes:\n",
    "            class_label, x, y, width, height = box.tolist()\n",
    "            class_label = int(class_label)\n",
    "\n",
    "            # 객체가 속한 grid cell의 행(=i)과 열(=j)을 계산한다.\n",
    "            i, j = int(self.S * y), int(self.S * x)\n",
    "            # cell 내에서 객체의 상대적인 좌표 계산.\n",
    "            x_cell, y_cell = self.S * x - j, self.S * y - i\n",
    "            \n",
    "            \"\"\"\n",
    "            cell 기준으로 bbox cell의 너비와 높이 계산.\n",
    "            \n",
    "            width_pixels = (width*self.image_width)\n",
    "            cell_pixels = (self.image_width)\n",
    "            \n",
    "            cell의 상대적인 너비를 찾는 방법 : width_pixels/cell_pixels\n",
    "            \"\"\"\n",
    "            width_cell, height_cell = (\n",
    "                width * self.S,\n",
    "                height * self.S,\n",
    "            )\n",
    "\n",
    "            # label_matrix가 현재 모든 값이 0인 상태이다.\n",
    "            # 이때 위에서 구한 grid cell에서의 객체 좌표에 대한 정보를 넣을 것이다.\n",
    "            # -> ground truth box 중심이 특정 cell에 존재할 경우 해당 cell의 C번째에 값을 1로 지정한다.\n",
    "            # C번째 : 객체가 존재하는지를 나타내는 index\n",
    "            if label_matrix[i, j, self.C] == 0:\n",
    "                # 객체가 있다면 1을 넣어준다.\n",
    "                label_matrix[i, j, self.C] = 1\n",
    "\n",
    "                # box 좌표\n",
    "                # cell 내에서의 bbox 좌표 및 너비를 텐서로 전환 후 저장.\n",
    "                box_coordinates = torch.tensor([\n",
    "                    x_cell, y_cell,\n",
    "                    width_cell, height_cell\n",
    "                    #min(width_cell, self.S - 1),\n",
    "                    #min(height_cell, self.S - 1)\n",
    "                ])\n",
    "                # C+1 ~ 23번째 index에 값 저장.\n",
    "                label_matrix[i, j, (self.C+1):(self.C+5)] = box_coordinates\n",
    "\n",
    "                # class_label에 대해 one-hot encoding 해주기.\n",
    "                # 값이 있는 cell을 1로 저장.\n",
    "                label_matrix[i, j, class_label] = 1\n",
    "                \n",
    "        return image, label_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8607c99c",
   "metadata": {},
   "source": [
    "### dataset 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "39abedd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = YoloDataset(\n",
    "    train_csv,\n",
    "    transform=transform,\n",
    "    img_dir=img_dir,\n",
    "    label_dir=label_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c3dfa12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label_matrix = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d8ac197a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f3fdaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5635436",
   "metadata": {},
   "source": [
    "# yolov1 network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a8c3b4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Information about architecture config:\n",
    "Tuple is structured : (kernel_size, filters, stride, padding) \n",
    "\"M\" is simply maxpooling with stride 2x2 and kernel 2x2\n",
    "List is structured by tuples and lastly int with number of repeats\n",
    "\"\"\"\n",
    "\n",
    "architecture_config = [\n",
    "    (7, 64, 2, 3),\n",
    "    \"M\",\n",
    "    \n",
    "    (3, 192, 1, 1),\n",
    "    \"M\",\n",
    "    \n",
    "    (1, 128, 1, 0),\n",
    "    (3, 256, 1, 1),\n",
    "    (1, 256, 1, 0),\n",
    "    (3, 512, 1, 1),\n",
    "    \"M\",\n",
    "    \n",
    "    [(1, 256, 1, 0), (3, 512, 1, 1), 4],   # -> conv 4번 반복\n",
    "    (1, 512, 1, 0),\n",
    "    (3, 1024, 1, 1),\n",
    "    \"M\",\n",
    "    \n",
    "    [(1, 512, 1, 0), (3, 1024, 1, 1), 2],  # -> conv 2번 반복\n",
    "    (3, 1024, 1, 1),\n",
    "    (3, 1024, 2, 1),\n",
    "    \n",
    "    (3, 1024, 1, 1),\n",
    "    (3, 1024, 1, 1),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f81242",
   "metadata": {},
   "source": [
    "### CNN block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1166005f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(CNNBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        self.leakyrelu = nn.LeakyReLU(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.leakyrelu(self.batchnorm(self.conv(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07e998d",
   "metadata": {},
   "source": [
    "# Yolov1 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "227f6229",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Yolov1(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels=3, **kwargs):\n",
    "        super(Yolov1, self).__init__()\n",
    "        self.architecture = architecture_config\n",
    "        self.in_channels = in_channels\n",
    "        self.darknet = self._create_conv_layers(self.architecture)\n",
    "        # 해당 top구조를 darknet구조라 한다.\n",
    "        self.fcs = self._create_fcs(**kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.darknet(x)\n",
    "        return self.fcs(torch.flatten(x, start_dim=1))\n",
    "\n",
    "    def _create_conv_layers(self, architecture):\n",
    "        layers = []\n",
    "        in_channels = self.in_channels\n",
    "\n",
    "        for x in architecture:\n",
    "            # arch_config에서 값이 tuple인 경우 : (7, 64, 2, 3)\n",
    "            if type(x) == tuple:\n",
    "                layers += [\n",
    "                    CNNBlock(\n",
    "                        in_channels, x[1], kernel_size=x[0], stride=x[2], padding=x[3],\n",
    "                    )\n",
    "                ]\n",
    "                in_channels = x[1]\n",
    "                \n",
    "            # arch_config에서 값이 string인 경우 : 'M'\n",
    "            # -> maxpooling\n",
    "            elif type(x) == str:\n",
    "                layers += [nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))]\n",
    "                \n",
    "            # arch_config에서 값이 list인 경우 : [(1, 256, 1, 0), (3, 512, 1, 1), 4]\n",
    "            elif type(x) == list:\n",
    "                conv1 = x[0]\n",
    "                conv2 = x[1]\n",
    "                num_repeats = x[2]\n",
    "                \n",
    "                # 2번 혹은 4번 돌아감.\n",
    "                for _ in range(num_repeats):\n",
    "                    \n",
    "                    # 1x1 conv인 경우\n",
    "                    layers += [\n",
    "                        CNNBlock(\n",
    "                            in_channels,\n",
    "                            conv1[1],\n",
    "                            kernel_size=conv1[0],\n",
    "                            stride=conv1[2],\n",
    "                            padding=conv1[3],\n",
    "                        )\n",
    "                    ]\n",
    "                    \n",
    "                    # 3x3 conv인 경우\n",
    "                    layers += [\n",
    "                        CNNBlock(\n",
    "                            conv1[1],\n",
    "                            conv2[1],\n",
    "                            kernel_size=conv2[0],\n",
    "                            stride=conv2[2],\n",
    "                            padding=conv2[3],\n",
    "                        )\n",
    "                    ]\n",
    "                    in_channels = conv2[1]\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _create_fcs(self, split_size, num_boxes, num_classes):\n",
    "        S, B, C = split_size, num_boxes, num_classes\n",
    "\n",
    "        # In original paper this should be\n",
    "        # nn.Linear(1024*S*S, 4096),\n",
    "        # nn.LeakyReLU(0.1),\n",
    "        # nn.Linear(4096, S*S*(B*5+C))\n",
    "\n",
    "        return nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024 * S * S, 496),\n",
    "            nn.Dropout(0.0),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(496, S * S * (C + B * 5)),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "38268c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Yolov1(split_size=7, num_boxes=2, num_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def3bb9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "877e2084",
   "metadata": {},
   "source": [
    "# LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "de6bdd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, S=7, B=2, C=4):\n",
    "        super(YoloLoss, self).__init__()\n",
    "        self.mse = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "        \"\"\"\n",
    "        S is split size of image (in paper 7),\n",
    "        B is number of boxes (in paper 2),\n",
    "        C is number of classes (in paper and VOC dataset is 4),\n",
    "        \"\"\"\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "\n",
    "        # 가중치 파라미터\n",
    "        # pay loss for no object (noobj) and the box coordinates (coord)\n",
    "        self.lambda_noobj = 0.5\n",
    "        self.lambda_coord = 5   # 상자 좌표\n",
    "        \n",
    "    # 손실함수 계산 시작\n",
    "    # train 시킬 때 image(=x)를 model에 넣어 나온 예측값이 out이다.\n",
    "    # predictions = out, target = y(data loader) = label_matrix(YoloDataset)\n",
    "    def forward(self, predictions, target):\n",
    "        '''\n",
    "        각 grid cell마다 2개의 bbox를 예측하고,\n",
    "        그 중 confidence score가 높은 1개의 bbox를 학습에 적용.\n",
    "        '''\n",
    "        \n",
    "        # grid cell 형태로 예측값들을 다시 배치하기 위해서 pred를 재구조화한다.\n",
    "        '''\n",
    "        predictions 입력 시 shape : (BATCH_SIZE, S, S, (C+B*5)) => 7x7x14\n",
    "        \n",
    "        [..., :C] = 각 클래스에 대한 확률  /  [..., C:C+4] : 첫 번째 bbox에 대한 좌표 및 너비(x, y, w, h)\n",
    "        [..., C+4:C+5] : 첫 번째 bbox에 대한 confidence score\n",
    "        \n",
    "        [..., C+6:C+9] : 두 번째 bbox에 대한 좌표 및 너비(x, y, w, h)\n",
    "        [..., C+9:C+10] : 두 번째 bbox에 대한 confidence score\n",
    "        \n",
    "        target 입력 시 shape : (BATCH_SIZE, S, S, (C+B*5))  => 7x7x14\n",
    "        [..., :4] : 각 target bbox 실제 좌표 및 너비(x, y, w, h)\n",
    "        [..., 4:5] : 각 target bbox에 대한 존재 여부 (1=객체있음 , 0=객체없음)\n",
    "        [..., 5:C+5] : 각 target bbox에 대한 class 정보로 one-hot encoding된 벡터.\n",
    "                       ex) 만약 class=2 -> [0, 1, 0, 0]으로 표현\n",
    "        [..., C+5:(C+B*5)] : \n",
    "        '''\n",
    "        \n",
    "        # predictions : 모델의 출력으로 받은 예측 값(SxSx(C+B*5)) 크기의 feature map을 flatten한 결과.\n",
    "        # target : 실제 target 값. 모델이 예측하려는 bbox와 클래스 정보 포함.\n",
    "        predictions = predictions.reshape(-1, self.S, self.S, self.C + self.B * 5)\n",
    "        \n",
    "        # intersection_over_union -> utils.ipybn 파일에 있는 함수.\n",
    "        # 해당 함수를 사용해 target bbox로 예측된 두 개의 bbox 좌표에 대한 IoU를 계산한다.\n",
    "        # prediction에서 첫번째 bbox 좌표\n",
    "        iou_b1 = intersection_over_union(predictions[..., (self.C+1):(self.C+5)], target[..., (self.C+1):(self.C+5)])\n",
    "        # prediction에서 두번째 bbox 좌표\n",
    "        iou_b2 = intersection_over_union(predictions[..., (self.C+6):(self.C+self.B * 5)], target[..., (self.C+1):(self.C+5)])\n",
    "        ious = torch.cat([iou_b1, iou_b2], dim=0)\n",
    "        \n",
    "        # ious에서 두 예측 중에서 IoU가 가장 높은 상자를 선택\n",
    "        # 해당 값은 변수 bestbox에 저장한다.\n",
    "        # 이때 bestbox는 IoU가 더 높은 box의 index가 저장이 된다.\n",
    "        iou_maxes, bestbox = torch.max(ious, dim=0)\n",
    "        # exists_box : target의 마지막 차원에서 bbox 존재 여부 나타내는 값 저장.\n",
    "        # grid cell에 ground truth box 중심이 존재하는지 여부 확인.\n",
    "        # 1 = 중심이 있음  ,  0 = 중심이 없음.\n",
    "        exists_box = target[..., self.C]#.unsqueeze(-1)  # in paper this is Iobj_i\n",
    "\n",
    "        \n",
    "        # ======================== #\n",
    "        #     Localizaton loss     #\n",
    "        # ======================== #\n",
    "\n",
    "        # object 없는 상자를 0으로 설정.\n",
    "        # 두 prediction 중 이전에 계산 된 IoU에서 가장 높은 예측 중 하나만 꺼낸다.\n",
    "        box_predictions = exists_box * (\n",
    "            (\n",
    "                bestbox * predictions[..., (self.C + 6):(self.C + self.B * 5)]\n",
    "                + (1 - bestbox) * predictions[..., (self.C + 1):(self.C + 5)]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        box_targets = exists_box * target[..., (self.C+1):(self.C+5)]\n",
    "        \n",
    "        # Take sqrt of width, height of boxes to ensure that\n",
    "        box_predictions[..., 2:4] = torch.sign(box_predictions[..., 2:4]) * torch.sqrt(\n",
    "            torch.abs(box_predictions[..., 2:4] + 1e-6)\n",
    "        )\n",
    "        box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])\n",
    "        \n",
    "        # IoU기준으로 선택된 bbox 사용해 pred_bbox와 실제 bbox 간의 좌표 손실 계산.\n",
    "        box_loss = self.mse(\n",
    "            torch.flatten(box_predictions, end_dim=-2),\n",
    "            torch.flatten(box_targets, end_dim=-2),\n",
    "        )\n",
    "\n",
    "        \n",
    "        # ==================== #\n",
    "        #    Confidence loss   #\n",
    "        #   FOR OBJECT LOSS    #\n",
    "        # ==================== #\n",
    "\n",
    "        # pred_box : IoU가 가장 높은 bbox의 신뢰도 점수\n",
    "        pred_box = (\n",
    "            bestbox * predictions[..., (self.C+5):(self.C+6)] + (1 - bestbox) * predictions[..., (self.C):(self.C+1)]\n",
    "        )\n",
    "        \n",
    "        # IoU기준으로 선택된 bbox에 해당하는 신뢰도 점수 사용하여 계산.\n",
    "        object_loss = self.mse(\n",
    "            torch.flatten(exists_box * pred_box),\n",
    "            torch.flatten(exists_box * target[..., (self.C):(self.C+1)]), # --> ?\n",
    "        )\n",
    "\n",
    "        \n",
    "        # ======================= #\n",
    "        #   FOR NO OBJECT LOSS    #\n",
    "        # ======================= #\n",
    "\n",
    "        # max_no_obj = torch.max(predictions[..., 20:21], predictions[..., 25:26])\n",
    "        # no_object_loss = self.mse(\n",
    "        #    torch.flatten((1 - exists_box) * max_no_obj, start_dim=1),\n",
    "        #    torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1),\n",
    "        #)\n",
    "\n",
    "        # 비객체 손실 계산 : 객체가 없는 grid cell에 대한 손실 계산.\n",
    "        # -> 신뢰도 점수에 대한 loss이고, 모델이 객체가 없는 위치에 대한 신뢰도를 낮출 수 있게 도와준다.\n",
    "        # 객체가 없을 경우 두 bbox를 모두 학습에 참여한다.\n",
    "        no_object_loss = self.mse(\n",
    "            torch.flatten((1 - exists_box) * predictions[..., (self.C):(self.C+1)], start_dim=1),\n",
    "            torch.flatten((1 - exists_box) * target[..., (self.C):(self.C+1)], start_dim=1)  # --> ?\n",
    "        )\n",
    "        \n",
    "        no_object_loss += self.mse(\n",
    "            torch.flatten((1 - exists_box) * predictions[..., (self.C+5):(self.C+6)], start_dim=1),\n",
    "            torch.flatten((1 - exists_box) * target[..., (self.C+5):(self.C+6)], start_dim=1)  # --> ?\n",
    "        )\n",
    "        \n",
    "        # ================== #\n",
    "        #     CLASS LOSS     #\n",
    "        # ================== #\n",
    "        # 클래스 손실 게산 : 예측된 클래스 확률과 실제 클래스 정보 사이의 loss를 계산한다.\n",
    "        # -> C개의 class score를 target과 비교해 mse loss를 구한다.\n",
    "        class_loss = self.mse(\n",
    "            torch.flatten(exists_box * predictions[..., :self.C], end_dim=-2,),\n",
    "            torch.flatten(exists_box * target[..., :self.C], end_dim=-2,),\n",
    "        )\n",
    "        \n",
    "        # box_loss , object_loss , no_object_loss , class_loss를 전부 더하기.\n",
    "        loss = (\n",
    "            self.lambda_coord * box_loss  # first two rows in paper\n",
    "            + object_loss  # third row in paper\n",
    "            + self.lambda_noobj * no_object_loss  # forth row\n",
    "            + class_loss  # fifth row\n",
    "        )\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d898d34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf947d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "S=7, B=2, C=4\n",
    "\n",
    "# 가중치 파라미터\n",
    "# pay loss for no object (noobj) and the box coordinates (coord)\n",
    "lambda_noobj = 0.5\n",
    "lambda_coord = 5   # 상자 좌표\n",
    "\n",
    "# 손실함수 계산 시작\n",
    "predictions = out\n",
    "target = y  # 값이 모두 0인 7x7x14 tensor넣어주기.\n",
    "'''\n",
    "각 grid cell마다 2개의 bbox를 예측하고,\n",
    "그 중 confidence score가 높은 1개의 bbox를 학습에 적용.\n",
    "'''\n",
    "\n",
    "# predictions : 모델의 출력으로 받은 예측 값(SxSx(C+B*5)) 크기의 feature map을 flatten한 결과.\n",
    "# target : 실제 target 값. 모델이 예측하려는 bbox와 클래스 정보 포함.\n",
    "predictions = predictions.reshape(-1, self.S, self.S, self.C + self.B * 5)\n",
    "\n",
    "iou_b1 = intersection_over_union(predictions[..., (self.C+1):(self.C+5)], target[..., (self.C+1):(self.C+5)])\n",
    "# prediction에서 두번째 bbox 좌표\n",
    "iou_b2 = intersection_over_union(predictions[..., (self.C+6):(self.C+self.B * 5)], target[..., (self.C+1):(self.C+5)])\n",
    "ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim=0)\n",
    "\n",
    "iou_maxes, bestbox = torch.max(ious, dim=0)\n",
    "# exists_box : target의 마지막 차원에서 bbox 존재 여부 나타내는 값 저장.\n",
    "# grid cell에 ground truth box 중심이 존재하는지 여부 확인.\n",
    "# 1 = 중심이 있음  ,  0 = 중심이 없음.\n",
    "exists_box = target[..., self.C].unsqueeze(-1)  # in paper this is Iobj_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "afb7ce18",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 7, 7, 14])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c287c17e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47cb0a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bd703c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fd77134",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e7b8e0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = YoloDataset(\n",
    "    train_csv,\n",
    "    transform=transform,\n",
    "    img_dir=img_dir,\n",
    "    label_dir=label_dir\n",
    ")\n",
    "\n",
    "train, vali = train_test_split(dataset, test_size=0.8, random_state=123)  # 80\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=0,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "vali_loader = DataLoader(\n",
    "    dataset=vali,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=0,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "71a56311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.3397, 0.5686, 1.6478, 2.7615,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[3, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfe5b01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b228f808",
   "metadata": {},
   "source": [
    "### train loader에 값이 제대로 있는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6aa35507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "846747a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 448, 448])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "images.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1d322f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a561c02",
   "metadata": {},
   "source": [
    "# model 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "6b533c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Yolov1(split_size=7, num_boxes=2, num_classes=4).to(DEVICE)\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(), lr=0.1, weight_decay=0\n",
    ")\n",
    "loss_fn = YoloLoss()\n",
    "\n",
    "if LOAD_MODEL:\n",
    "    load_checkpoint(torch.load(LOAD_MODEL_FILE), model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2080dd8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da6e28b4",
   "metadata": {},
   "source": [
    "# 훈련 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "2695a5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(train_loader, model, optimizer, loss_fn):\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    mean_loss = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x, y) in enumerate(loop):\n",
    "            # dataset에서 return한 image, label_matrix값.\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            out = model(x)\n",
    "\n",
    "            loss = loss_fn(out, y)\n",
    "            print('Loss : ', loss)\n",
    "            mean_loss.append(loss.item())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # update progress bar\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        print(f\"Mean loss was {sum(mean_loss)/len(mean_loss)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "53c8dc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  1\n",
      "Train mAP: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|                                                                                            | 0/1 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (7) must match the size of tensor b (4) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13196\\3058602868.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Train mAP: {mean_avg_prec}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mtrain_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13196\\3336839112.py\u001b[0m in \u001b[0;36mtrain_fn\u001b[1;34m(train_loader, model, optimizer, loss_fn)\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loss : '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[0mmean_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13196\\1812713578.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, predictions, target)\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;31m# object 없는 상자를 0으로 설정.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;31m# 두 prediction 중 이전에 계산 된 IoU에서 가장 높은 예측 중 하나만 꺼낸다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         box_predictions = exists_box * (\n\u001b[0m\u001b[0;32m     74\u001b[0m             (\n\u001b[0;32m     75\u001b[0m                 \u001b[0mbestbox\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mC\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mC\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mB\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (7) must match the size of tensor b (4) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(1)):\n",
    "\n",
    "    print('epoch : ', epoch + 1)\n",
    "\n",
    "    pred_boxes, target_boxes = get_bboxes(\n",
    "        train_loader, model, iou_threshold=0.3, threshold=0.3\n",
    "    )\n",
    "\n",
    "    mean_avg_prec = mean_average_precision(\n",
    "        pred_boxes, target_boxes, iou_threshold=0.5, box_format=\"midpoint\"\n",
    "    )\n",
    "    print(f\"Train mAP: {mean_avg_prec}\")\n",
    "\n",
    "    train_fn(train_loader, model, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b5599b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d0a89c1",
   "metadata": {},
   "source": [
    "### 에러내용 : RuntimeError: shape '[16, 7, 7, 30]' is invalid for input of size 10976\n",
    "<br>\n",
    "발생이유<br><br>\n",
    "input shape이 [16, 7, 7, 30]이어야 하는데, 내가 사용한 input size는 [16, 7, 7, 14]이다.<br>\n",
    "yolo architecture에서 마지막 부분은 7x7x30이기 때문에 이에 맞추기..<br><br>\n",
    "\n",
    "## B:5, C:5로 변경해서 하기!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8995bf0b",
   "metadata": {},
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c218eec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IoU를 계산하는 함수.\n",
    "# 계산 결과 : 겹치는 부분이 마이너스라서 모든 값이 다 0이 나옴.\n",
    "# => 예측과 실제가 겹쳐지는 부분이 매우 미세하거나 없음.\n",
    "def intersection_over_union(boxes_preds, boxes_labels, box_format=\"midpoint\"):\n",
    "    \"\"\"\n",
    "    Calculates intersection over union\n",
    "\n",
    "    Parameters:\n",
    "        boxes_preds (tensor): 예측된 bbox 좌표 정보 가지고 있음. (BATCH_SIZE, 4)\n",
    "        boxes_labels (tensor): 실제 label bbox와 좌표 정보 가지고 있음. (BATCH_SIZE, 4)\n",
    "        box_format (str): midpoint(중심점, w, h) / corners(좌상단 , 우하단), if boxes (x,y,w,h) or (x1,y1,x2,y2)\n",
    "\n",
    "    Returns:\n",
    "        tensor: Intersection over union for all examples\n",
    "    \"\"\"\n",
    "\n",
    "    if box_format == \"midpoint\":\n",
    "        # box_1\n",
    "        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
    "        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
    "        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
    "        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
    "        # box_2\n",
    "        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
    "        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
    "        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
    "        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
    "\n",
    "    if box_format == \"corners\":\n",
    "        # box_1\n",
    "        box1_x1 = boxes_preds[..., 0:1]\n",
    "        box1_y1 = boxes_preds[..., 1:2]\n",
    "        box1_x2 = boxes_preds[..., 2:3]\n",
    "        box1_y2 = boxes_preds[..., 3:4]  # (N, 1)\n",
    "        # box_2\n",
    "        box2_x1 = boxes_labels[..., 0:1]\n",
    "        box2_y1 = boxes_labels[..., 1:2]\n",
    "        box2_x2 = boxes_labels[..., 2:3]\n",
    "        box2_y2 = boxes_labels[..., 3:4]\n",
    "\n",
    "    x1 = torch.max(box1_x1, box2_x1)\n",
    "    y1 = torch.max(box1_y1, box2_y1)\n",
    "    x2 = torch.min(box1_x2, box2_x2)\n",
    "    y2 = torch.min(box1_y2, box2_y2)\n",
    "\n",
    "    # 박스의 좌표를 비교해 겹치는 영역을 구해야한다.\n",
    "    # clamp(0) : 영역이 음수가 되지 않도록 해 영역이 겹치치 않는 경우를 처리.\n",
    "    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
    "    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
    "    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
    "\n",
    "    return intersection / (box1_area + box2_area - intersection + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "83f4659d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        # dataset에서 return한 image, label_matrix값.\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        out = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "0eae5616",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out.reshape(-1, 7, 7, 4 + 2 * 5)\n",
    "a = intersection_over_union(out, y, box_format=\"midpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "9e206d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-3.7014e-02, -2.5416e-02,  7.0422e-02,  ...,  2.5490e-01,\n",
       "            9.9237e-02,  1.2393e-01],\n",
       "          [ 2.7664e-01,  7.9222e-02,  7.4289e-02,  ..., -2.0881e-01,\n",
       "           -1.0846e-01,  3.3345e-01],\n",
       "          [ 1.2530e-01,  7.7614e-02, -2.0971e-02,  ...,  2.6625e-01,\n",
       "            4.3843e-02, -2.8554e-01],\n",
       "          ...,\n",
       "          [ 4.2469e-01,  2.6156e-01, -1.6536e-01,  ...,  1.7936e-01,\n",
       "            5.7187e-02,  5.7697e-02],\n",
       "          [-1.4052e-01, -1.1648e-01,  3.3048e-01,  ..., -2.4145e-01,\n",
       "           -4.4412e-01,  3.0446e-01],\n",
       "          [ 2.5341e-02, -1.2086e-02, -2.8258e-02,  ..., -1.5466e-01,\n",
       "           -3.2301e-01,  1.1946e-01]],\n",
       "\n",
       "         [[ 1.6862e-01, -1.6228e-01,  3.8125e-01,  ..., -3.1161e-01,\n",
       "           -1.2866e-01,  3.6836e-03],\n",
       "          [-9.6899e-02,  2.6513e-01,  1.3100e-01,  ..., -3.9063e-02,\n",
       "            4.4585e-01,  1.0860e-02],\n",
       "          [-3.4279e-01, -3.6383e-01, -1.0719e-01,  ..., -6.5599e-01,\n",
       "            1.6501e-01,  4.4188e-01],\n",
       "          ...,\n",
       "          [-1.7514e-01,  1.2787e-01,  1.0092e-01,  ..., -6.9843e-02,\n",
       "           -2.9460e-01,  1.7808e-01],\n",
       "          [ 3.9515e-02,  1.4568e-02,  2.5504e-01,  ...,  2.9717e-01,\n",
       "            1.4987e-01,  7.9379e-02],\n",
       "          [-1.3610e-01, -1.0938e-01, -7.0668e-02,  ..., -1.0856e-01,\n",
       "           -4.3850e-01, -1.3590e-02]],\n",
       "\n",
       "         [[-8.0110e-02, -9.0936e-02,  1.7799e-01,  ...,  1.8401e-01,\n",
       "            3.4958e-01, -1.7456e-01],\n",
       "          [ 1.6841e-02,  1.5363e-01,  2.7279e-01,  ..., -2.3257e-02,\n",
       "            1.4533e-02, -8.5672e-02],\n",
       "          [ 7.9667e-02, -1.7886e-01,  2.5941e-01,  ..., -1.9298e-01,\n",
       "            9.1611e-02,  4.3041e-02],\n",
       "          ...,\n",
       "          [-1.8849e-01, -9.5537e-02, -1.3395e-01,  ...,  1.7893e-01,\n",
       "           -2.7635e-02, -1.4294e-01],\n",
       "          [ 2.5703e-02, -1.3492e-01,  3.2378e-01,  ...,  8.7415e-02,\n",
       "           -3.0916e-02, -1.7989e-02],\n",
       "          [-2.9081e-02, -4.1970e-01, -1.5393e-02,  ...,  9.4852e-02,\n",
       "           -2.6366e-01, -1.4831e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.9389e-01,  5.7034e-02,  2.9478e-01,  ..., -3.6136e-01,\n",
       "           -2.1148e-01, -1.1797e-01],\n",
       "          [-2.7241e-03, -2.2587e-01,  2.2841e-01,  ...,  2.0336e-01,\n",
       "            1.5917e-01, -2.5294e-01],\n",
       "          [ 1.6662e-01,  5.4730e-01, -4.7153e-01,  ...,  3.0474e-01,\n",
       "            6.6265e-02, -6.5496e-02],\n",
       "          ...,\n",
       "          [-1.1777e-01, -1.4045e-01,  1.1569e-02,  ..., -2.8091e-02,\n",
       "           -4.6800e-01,  4.0599e-01],\n",
       "          [-2.0137e-01,  3.1784e-01,  3.3971e-01,  ...,  2.8974e-01,\n",
       "           -2.0974e-01,  3.1706e-01],\n",
       "          [ 4.3564e-01, -3.5583e-01, -9.9704e-02,  ..., -1.0235e-01,\n",
       "            5.0399e-01,  2.3698e-01]],\n",
       "\n",
       "         [[-1.1591e-02, -7.9270e-02,  8.9240e-02,  ..., -1.6970e-01,\n",
       "           -4.6160e-01, -1.7915e-01],\n",
       "          [ 2.1789e-01,  1.6298e-01,  3.7804e-01,  ...,  4.1029e-02,\n",
       "            3.0859e-01, -6.5257e-02],\n",
       "          [ 3.6020e-01,  2.2444e-01,  2.1515e-01,  ..., -1.2191e-01,\n",
       "            2.4056e-02, -1.9395e-01],\n",
       "          ...,\n",
       "          [ 1.3524e-01,  3.7293e-01,  5.6659e-03,  ..., -1.3308e-01,\n",
       "            5.4180e-01, -3.6223e-01],\n",
       "          [ 1.0036e-01, -1.1637e-01,  2.0776e-01,  ..., -2.7283e-01,\n",
       "            9.3560e-02, -3.6180e-01],\n",
       "          [ 2.8655e-01, -3.1299e-01,  3.7418e-01,  ...,  3.2318e-01,\n",
       "            1.2926e-01,  2.2457e-01]],\n",
       "\n",
       "         [[-1.7581e-01, -3.7458e-01, -1.5626e-01,  ..., -2.7134e-01,\n",
       "           -3.2595e-03,  1.7783e-01],\n",
       "          [ 3.0454e-01,  1.1600e-01, -6.6350e-02,  ..., -1.4085e-02,\n",
       "           -1.3050e-01,  7.7037e-02],\n",
       "          [ 2.7107e-01,  2.1526e-01, -6.3038e-02,  ..., -1.2075e-01,\n",
       "           -2.3588e-01,  2.9367e-01],\n",
       "          ...,\n",
       "          [-1.0904e-01, -1.0307e-01, -3.8994e-01,  ..., -4.1119e-03,\n",
       "            2.0612e-01,  1.2874e-01],\n",
       "          [ 1.3711e-01, -2.0093e-01,  3.5378e-02,  ...,  3.2048e-02,\n",
       "            1.9412e-02,  5.9672e-02],\n",
       "          [-2.7504e-01, -2.4415e-01, -2.1677e-01,  ..., -3.9433e-02,\n",
       "            1.6065e-01, -1.1482e-01]]],\n",
       "\n",
       "\n",
       "        [[[-3.2291e-02, -1.5662e-01,  8.0559e-02,  ..., -4.8375e-03,\n",
       "           -2.6877e-02,  1.0259e-01],\n",
       "          [ 2.0774e-02,  2.8291e-01,  4.1583e-03,  ...,  2.4992e-02,\n",
       "            1.3887e-01,  1.8696e-01],\n",
       "          [-2.1913e-02, -1.4542e-02, -2.1983e-02,  ...,  2.1315e-01,\n",
       "            5.9547e-02, -1.7849e-01],\n",
       "          ...,\n",
       "          [ 1.8588e-01, -6.8793e-02, -1.0037e-01,  ..., -1.0767e-01,\n",
       "            9.5809e-02,  7.4368e-03],\n",
       "          [-1.8306e-01,  3.4435e-02,  2.0838e-01,  ..., -1.5214e-01,\n",
       "           -2.3273e-01,  3.9064e-02],\n",
       "          [ 5.1160e-02,  5.0705e-03,  1.1020e-01,  ..., -1.5201e-01,\n",
       "           -1.0155e-01,  6.3658e-02]],\n",
       "\n",
       "         [[-1.8565e-01, -3.9685e-02, -1.2283e-01,  ..., -1.9935e-02,\n",
       "            5.0230e-02,  1.2760e-02],\n",
       "          [ 9.6034e-02, -9.9446e-02, -8.0775e-02,  ..., -3.5435e-02,\n",
       "            2.1711e-01, -3.9834e-03],\n",
       "          [-2.0702e-01, -7.2844e-02,  1.3296e-01,  ..., -1.1692e-01,\n",
       "            9.1649e-02, -2.2628e-01],\n",
       "          ...,\n",
       "          [ 4.8757e-02, -3.5394e-02, -1.2682e-02,  ..., -9.6181e-02,\n",
       "           -1.0727e-02,  1.5125e-01],\n",
       "          [ 3.2764e-02, -1.2000e-01,  2.8044e-02,  ..., -7.8111e-02,\n",
       "            3.2914e-02, -1.0168e-01],\n",
       "          [ 3.8521e-02, -4.0066e-03, -4.5510e-02,  ..., -6.4405e-02,\n",
       "           -1.2619e-01, -1.5332e-01]],\n",
       "\n",
       "         [[-4.8556e-02, -2.6641e-01,  1.9197e-01,  ...,  7.7837e-02,\n",
       "            2.0624e-01,  4.3257e-02],\n",
       "          [ 2.3755e-03,  1.3198e-01,  1.2153e-01,  ..., -1.3809e-01,\n",
       "            1.6052e-01,  3.4218e-02],\n",
       "          [ 6.1262e-02, -1.5804e-01, -1.8637e-01,  ..., -2.3598e-01,\n",
       "            4.3883e-02, -3.9108e-02],\n",
       "          ...,\n",
       "          [-1.1581e-01, -8.8223e-03,  1.1565e-01,  ..., -2.2219e-02,\n",
       "           -2.2239e-01,  6.4387e-02],\n",
       "          [ 1.5077e-01, -5.7796e-02,  1.2878e-01,  ..., -1.3549e-01,\n",
       "            9.7969e-02,  1.2666e-01],\n",
       "          [ 2.1155e-01, -1.0916e-01, -2.7852e-02,  ..., -6.7983e-03,\n",
       "            5.7258e-03, -6.9196e-03]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.7530e-02, -9.6773e-03,  1.5318e-01,  ...,  2.8526e-02,\n",
       "            7.6558e-02,  1.3070e-01],\n",
       "          [ 5.0630e-02,  1.5858e-01,  1.7552e-01,  ...,  2.1140e-02,\n",
       "            1.8267e-01, -6.2912e-02],\n",
       "          [ 3.7637e-02,  9.4685e-02, -1.5023e-01,  ...,  6.0388e-02,\n",
       "            7.4657e-02, -1.7385e-01],\n",
       "          ...,\n",
       "          [-1.5250e-01, -3.1677e-01,  3.0286e-02,  ..., -1.6507e-02,\n",
       "           -4.4643e-02,  2.5899e-01],\n",
       "          [-8.7861e-02,  2.0353e-01,  6.6178e-02,  ...,  5.3831e-02,\n",
       "           -1.3719e-01, -5.6906e-03],\n",
       "          [ 1.3858e-01, -3.0987e-02, -1.8593e-01,  ...,  4.4008e-02,\n",
       "            1.6088e-01, -4.3787e-02]],\n",
       "\n",
       "         [[-8.4473e-02,  2.2154e-02,  1.4093e-01,  ...,  2.0859e-02,\n",
       "           -1.7210e-01, -7.4124e-02],\n",
       "          [-5.3289e-02, -9.9567e-02,  6.4143e-02,  ..., -1.5147e-01,\n",
       "            3.2305e-01, -3.2634e-02],\n",
       "          [ 1.0188e-01,  1.4547e-01, -1.0668e-02,  ..., -7.1081e-02,\n",
       "           -2.6501e-02, -2.1909e-01],\n",
       "          ...,\n",
       "          [ 7.4959e-02,  2.1140e-01,  7.9014e-02,  ..., -2.2943e-01,\n",
       "           -7.6974e-02, -2.3089e-02],\n",
       "          [-1.7890e-01,  4.1082e-02,  1.5690e-02,  ..., -3.0907e-02,\n",
       "            2.6917e-02, -1.2881e-01],\n",
       "          [ 1.3411e-01, -8.9427e-03, -6.8215e-02,  ...,  6.6332e-02,\n",
       "            1.7169e-01,  1.3017e-01]],\n",
       "\n",
       "         [[-1.7018e-01, -1.4983e-01, -1.4079e-01,  ..., -6.2880e-02,\n",
       "           -3.2174e-02,  1.0140e-01],\n",
       "          [ 1.1591e-01, -3.2972e-02, -9.5307e-02,  ..., -1.3006e-01,\n",
       "           -6.8535e-02,  1.4983e-01],\n",
       "          [-8.3373e-02,  3.3261e-02,  5.9187e-02,  ...,  1.4535e-01,\n",
       "            7.0320e-02,  2.2796e-01],\n",
       "          ...,\n",
       "          [-2.4972e-01,  7.3453e-02, -1.0236e-01,  ...,  1.9917e-02,\n",
       "            2.1684e-01, -4.5768e-03],\n",
       "          [-3.1848e-02,  3.3013e-02, -9.5034e-02,  ..., -2.9661e-03,\n",
       "           -2.5791e-01,  1.1901e-01],\n",
       "          [-8.7607e-02,  4.8188e-02, -1.7979e-01,  ..., -1.3315e-01,\n",
       "            3.6333e-02, -2.1593e-02]]],\n",
       "\n",
       "\n",
       "        [[[-3.8744e-03, -6.6545e-02, -2.1643e-01,  ...,  8.2440e-02,\n",
       "            2.5169e-01,  1.8836e-01],\n",
       "          [ 2.1233e-01, -5.7762e-02,  7.6892e-02,  ..., -3.3588e-02,\n",
       "            1.3736e-01,  1.5663e-01],\n",
       "          [-7.9177e-02, -2.5536e-02, -9.9451e-02,  ...,  2.5094e-01,\n",
       "            6.8350e-03, -1.8045e-01],\n",
       "          ...,\n",
       "          [ 3.1451e-01, -8.2693e-02, -1.6993e-01,  ...,  6.5339e-02,\n",
       "            3.8070e-02, -3.3367e-02],\n",
       "          [-9.5793e-02, -8.1320e-02,  7.3513e-02,  ..., -3.5285e-01,\n",
       "           -2.7411e-01, -1.1478e-02],\n",
       "          [ 1.8777e-01,  5.9890e-02, -3.4546e-02,  ..., -1.7727e-01,\n",
       "           -2.6674e-01,  2.8606e-01]],\n",
       "\n",
       "         [[-8.6908e-02,  6.7473e-02, -1.0960e-01,  ..., -2.5281e-02,\n",
       "           -7.9076e-02, -8.1592e-02],\n",
       "          [-6.4085e-03,  7.2112e-02, -5.3522e-02,  ..., -8.2547e-02,\n",
       "            3.1257e-01, -7.4219e-02],\n",
       "          [-2.5103e-01, -3.8031e-01,  1.6494e-01,  ..., -2.0016e-01,\n",
       "            5.6502e-02, -5.4547e-03],\n",
       "          ...,\n",
       "          [-2.1763e-01, -2.4322e-01, -1.9879e-01,  ..., -1.7117e-01,\n",
       "           -2.6854e-01,  1.1872e-01],\n",
       "          [ 2.6414e-03,  2.1233e-02,  2.7951e-01,  ...,  1.7662e-01,\n",
       "            5.2101e-02, -2.3397e-01],\n",
       "          [ 1.5415e-02, -6.1864e-02, -6.1765e-02,  ..., -1.9107e-01,\n",
       "           -1.1555e-01, -1.7927e-01]],\n",
       "\n",
       "         [[-2.2942e-01, -2.1312e-01,  5.1675e-02,  ..., -7.0517e-02,\n",
       "            4.1717e-02, -2.7901e-01],\n",
       "          [ 3.4283e-04,  1.4941e-02,  1.7057e-01,  ..., -1.4141e-01,\n",
       "           -8.0912e-02, -6.5397e-02],\n",
       "          [ 7.4415e-02, -3.5085e-03, -8.5883e-02,  ..., -1.1080e-01,\n",
       "           -1.1145e-01,  4.6993e-02],\n",
       "          ...,\n",
       "          [ 4.1154e-03,  2.2242e-01, -1.1467e-01,  ...,  8.2111e-02,\n",
       "            5.1296e-02,  6.4312e-02],\n",
       "          [ 4.4291e-02, -6.8318e-02,  1.1608e-01,  ..., -9.6348e-02,\n",
       "           -4.4899e-02,  1.3676e-01],\n",
       "          [-2.5219e-01, -1.7839e-01,  1.2706e-01,  ...,  1.5618e-01,\n",
       "           -6.0724e-02,  6.4435e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-8.2794e-02, -1.1204e-01,  4.8806e-02,  ..., -1.0793e-01,\n",
       "           -1.5889e-01,  2.4068e-01],\n",
       "          [ 1.7098e-02,  2.4973e-01,  1.8236e-01,  ...,  2.1326e-01,\n",
       "           -4.0791e-03,  3.1467e-02],\n",
       "          [-3.1018e-02,  2.4340e-01, -9.3808e-02,  ...,  2.6050e-02,\n",
       "           -1.2482e-01, -2.1172e-01],\n",
       "          ...,\n",
       "          [ 5.1002e-02, -5.0572e-02,  7.5111e-02,  ...,  9.9401e-02,\n",
       "           -1.6775e-01,  5.2759e-01],\n",
       "          [-2.7172e-01,  1.4860e-01,  2.4593e-02,  ...,  2.5660e-02,\n",
       "           -2.6128e-01,  1.0244e-01],\n",
       "          [ 9.5155e-02, -2.7036e-01, -1.6813e-01,  ..., -3.9720e-02,\n",
       "            2.1045e-01, -2.6559e-01]],\n",
       "\n",
       "         [[-8.7538e-02, -8.6694e-02,  1.0750e-01,  ...,  5.4016e-02,\n",
       "           -1.2347e-01,  3.5663e-03],\n",
       "          [ 3.8555e-02, -5.1231e-02,  1.8866e-01,  ..., -5.8475e-02,\n",
       "            9.7488e-02,  2.9061e-02],\n",
       "          [ 1.8599e-02,  2.3641e-01,  1.6210e-01,  ..., -1.8998e-01,\n",
       "           -2.0705e-01, -1.9290e-01],\n",
       "          ...,\n",
       "          [ 2.2519e-01,  3.1527e-01,  2.4085e-01,  ..., -2.5402e-02,\n",
       "           -1.2017e-02, -1.6901e-01],\n",
       "          [-5.8924e-02, -1.5814e-01, -2.2948e-01,  ..., -4.3396e-02,\n",
       "            4.9915e-02, -1.0827e-01],\n",
       "          [ 1.5588e-01, -1.1183e-01,  2.3111e-01,  ...,  1.5192e-01,\n",
       "            3.7236e-01,  1.8796e-01]],\n",
       "\n",
       "         [[-8.3149e-02, -3.9025e-01,  3.8310e-02,  ...,  1.8475e-02,\n",
       "            5.4150e-02,  1.2914e-01],\n",
       "          [ 2.1476e-01, -1.9275e-02, -9.0910e-02,  ..., -2.0681e-01,\n",
       "           -3.7503e-02,  4.6080e-02],\n",
       "          [ 8.9095e-02, -5.0321e-02, -9.1710e-03,  ..., -6.4169e-02,\n",
       "           -1.4969e-01,  1.4723e-01],\n",
       "          ...,\n",
       "          [-1.7870e-01,  2.6169e-01, -1.9722e-01,  ..., -2.9346e-02,\n",
       "            1.5221e-01, -8.0041e-02],\n",
       "          [ 5.9003e-02, -1.0030e-01, -4.2538e-02,  ..., -8.2279e-02,\n",
       "           -8.5193e-03,  5.2337e-02],\n",
       "          [-3.0718e-01, -3.5160e-02,  5.0416e-02,  ..., -1.6718e-02,\n",
       "            8.1838e-02, -1.7190e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 4.7033e-03,  5.9175e-02, -1.2949e-01,  ..., -1.7444e-01,\n",
       "            1.8746e-01,  7.9967e-03],\n",
       "          [ 2.1231e-01,  3.4047e-02,  1.6690e-01,  ..., -5.9725e-02,\n",
       "            1.4155e-01,  7.8631e-02],\n",
       "          [ 6.2085e-02, -2.2025e-02,  3.9095e-02,  ...,  2.9520e-01,\n",
       "           -7.4847e-02, -6.9131e-02],\n",
       "          ...,\n",
       "          [ 3.7349e-01,  2.6490e-01, -2.7883e-01,  ...,  5.8524e-02,\n",
       "            7.4547e-03, -4.4175e-03],\n",
       "          [-1.8338e-01, -1.8892e-01,  1.9675e-01,  ..., -3.1001e-01,\n",
       "           -2.0614e-01,  3.7485e-01],\n",
       "          [ 2.7484e-01, -3.8603e-01, -1.3297e-01,  ..., -3.7737e-01,\n",
       "           -1.8397e-01,  6.0304e-02]],\n",
       "\n",
       "         [[-1.1142e-01, -5.0715e-02,  5.8700e-02,  ..., -5.8309e-02,\n",
       "            1.5099e-01, -3.0532e-03],\n",
       "          [ 1.7581e-01, -1.7693e-01, -1.8616e-01,  ..., -1.7619e-01,\n",
       "            3.1374e-01,  1.9059e-01],\n",
       "          [-7.9515e-02,  9.2774e-02,  1.9768e-01,  ...,  2.8656e-02,\n",
       "            9.1531e-03,  1.5617e-01],\n",
       "          ...,\n",
       "          [-1.0008e-01,  7.5837e-02, -1.1707e-01,  ...,  2.3021e-01,\n",
       "           -6.0977e-02,  1.9151e-01],\n",
       "          [ 1.1061e-01,  1.3828e-01, -1.1792e-01,  ..., -1.6985e-03,\n",
       "            9.2909e-02, -1.1013e-02],\n",
       "          [ 1.3386e-01, -2.3310e-01, -4.8001e-02,  ..., -2.0749e-01,\n",
       "           -1.6725e-01,  4.0580e-03]],\n",
       "\n",
       "         [[-1.4258e-01,  8.8905e-03,  1.9284e-01,  ..., -2.3305e-02,\n",
       "            3.0834e-01, -2.7671e-01],\n",
       "          [ 2.1084e-02,  1.7505e-01,  5.3557e-01,  ..., -3.6296e-01,\n",
       "            2.6264e-01, -6.6768e-02],\n",
       "          [ 1.4089e-01,  5.3424e-02, -4.6726e-02,  ..., -1.6265e-01,\n",
       "           -5.2864e-02,  1.3754e-01],\n",
       "          ...,\n",
       "          [-1.6603e-01, -3.5664e-02, -1.6829e-01,  ..., -2.4731e-01,\n",
       "           -1.4430e-01,  6.8337e-02],\n",
       "          [ 3.2737e-02,  4.4442e-03,  1.5602e-01,  ..., -1.3026e-01,\n",
       "           -1.0040e-01,  2.1627e-01],\n",
       "          [ 5.4379e-02, -2.6627e-01,  1.4471e-01,  ...,  7.6555e-02,\n",
       "           -5.8331e-02,  1.2712e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.8465e-01, -1.3227e-01,  2.5386e-01,  ..., -1.5666e-01,\n",
       "           -2.0951e-01,  2.6236e-01],\n",
       "          [ 2.3725e-01,  3.6815e-01,  1.2300e-01,  ...,  4.0409e-03,\n",
       "            3.9136e-01, -2.0489e-01],\n",
       "          [ 2.5179e-01,  3.1530e-01,  8.7257e-02,  ...,  2.2319e-01,\n",
       "            1.1558e-01, -1.0501e-01],\n",
       "          ...,\n",
       "          [-3.1963e-01, -7.5587e-02, -1.8052e-01,  ..., -3.9984e-02,\n",
       "           -3.4847e-01,  2.5743e-01],\n",
       "          [-2.4369e-01, -6.5355e-02,  3.6818e-05,  ...,  1.1355e-01,\n",
       "           -2.3176e-01, -1.4466e-02],\n",
       "          [ 3.0819e-02, -1.5334e-01, -2.4578e-01,  ..., -2.9263e-01,\n",
       "            1.4867e-01,  1.9814e-01]],\n",
       "\n",
       "         [[-5.9831e-02, -2.6841e-02, -2.0341e-01,  ...,  9.8707e-02,\n",
       "           -2.2764e-01,  1.0416e-02],\n",
       "          [-1.9567e-02, -2.4551e-01,  1.7783e-01,  ..., -3.0730e-02,\n",
       "           -1.6889e-01, -5.2811e-02],\n",
       "          [ 2.0748e-01,  4.7359e-01, -1.8739e-01,  ..., -1.7924e-01,\n",
       "           -4.7432e-02, -1.3090e-01],\n",
       "          ...,\n",
       "          [ 4.0204e-02,  5.1313e-01,  1.5792e-01,  ...,  1.3003e-01,\n",
       "           -1.9341e-01, -1.2540e-01],\n",
       "          [-3.3406e-02, -1.2946e-02, -2.5932e-03,  ..., -1.4220e-01,\n",
       "            5.3281e-02, -2.2981e-01],\n",
       "          [ 2.1960e-01, -1.4827e-01,  1.8889e-01,  ...,  1.3934e-01,\n",
       "            3.0314e-01, -1.3022e-02]],\n",
       "\n",
       "         [[-1.7659e-01, -9.5548e-02, -1.9840e-01,  ..., -2.1448e-01,\n",
       "           -8.7354e-02, -1.6324e-03],\n",
       "          [ 4.5393e-01, -3.9672e-02, -7.5402e-02,  ..., -2.8591e-02,\n",
       "           -8.1184e-02,  3.5196e-01],\n",
       "          [ 2.5169e-02, -7.8344e-02, -1.9723e-01,  ..., -1.1933e-01,\n",
       "            4.6950e-02,  9.7559e-02],\n",
       "          ...,\n",
       "          [-9.8328e-02,  3.5867e-01, -3.1823e-01,  ..., -1.8142e-01,\n",
       "            2.8058e-01,  1.8065e-01],\n",
       "          [ 1.0563e-01, -1.2832e-01,  5.1191e-02,  ...,  2.8464e-01,\n",
       "           -2.2113e-01,  1.7518e-01],\n",
       "          [-1.1871e-01, -1.4530e-01, -1.7468e-01,  ..., -3.9409e-02,\n",
       "            1.1246e-01,  8.8425e-02]]]], device='cuda:0')"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fa81c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "b801d0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NMS : 객체 탐지 결과에서 겹치는 예측 박스를 제거해 정확한 예측 결과 얻는데 사용한다.\n",
    "def non_max_suppression(bboxes, iou_threshold, threshold, box_format=\"corners\"):\n",
    "    \"\"\"\n",
    "    Does Non Max Suppression given bboxes\n",
    "\n",
    "    Parameters:\n",
    "        bboxes (list) : 객체 탐지 결과. 예측된 bbox 정보를 포함한 리스트이다.\n",
    "        shape = (클래스 예측, 확률점수, x1, y1, x2, y2)\n",
    "\n",
    "        iou_threshold (float) : IoU 임계값. 예측된 bbox들이 겹치는 정도를 평가하는데 사용된다.\n",
    "        IoU를 넘는 경우, 두 bbox 중 하나는 삭제\n",
    "        \n",
    "        threshold (float) : bbox 확률 점수가 이 임계값보다 작은 경우, 해당 bbox는 삭제.\n",
    "        \n",
    "        box_format (str): bbox 포맷을 나타내는 문자열.\n",
    "        midpoint = 중심 좌표와 width, height -> bbox 나타내는 것을 의미.\n",
    "        corners = 좌측 상단과 우측 하단의 좌표로 bbox 나타냄.\n",
    "\n",
    "    Returns:\n",
    "        list: bboxes after performing NMS given a specific IoU threshold\n",
    "    \"\"\"\n",
    "\n",
    "    assert type(bboxes) == list\n",
    "    \n",
    "    # 확률 점수가 threshold보다 미만이면 제외.\n",
    "    bboxes = [box for box in bboxes if box[1] > threshold]\n",
    "    # 내림차순을 정렬\n",
    "    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n",
    "    bboxes_after_nms = []\n",
    "    \n",
    "    while bboxes:\n",
    "        chosen_box = bboxes.pop(0)\n",
    "        \n",
    "        # 남아 있는 bbox 중에서 가장 확률이 높은 box선택.\n",
    "        # 이 box와 IoU가 iou_threshold를 초과하는 다른 box들 제거하고 결과를 반환한다.\n",
    "        bboxes = [\n",
    "            box\n",
    "            for box in bboxes\n",
    "            if box[0] != chosen_box[0]\n",
    "            or intersection_over_union(\n",
    "                torch.tensor(chosen_box[2:]),\n",
    "                torch.tensor(box[2:]),\n",
    "                box_format=box_format,\n",
    "            )\n",
    "            < iou_threshold\n",
    "        ]\n",
    "\n",
    "        bboxes_after_nms.append(chosen_box)\n",
    "\n",
    "    return bboxes_after_nms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ddb0d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "484c99e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_average_precision(\n",
    "    pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\", num_classes=20\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates mean average precision \n",
    "\n",
    "    Parameters:\n",
    "        pred_boxes (list): list of lists containing all bboxes with each bboxes\n",
    "        specified as [train_idx, class_prediction, prob_score, x1, y1, x2, y2]\n",
    "        true_boxes (list): Similar as pred_boxes except all the correct ones \n",
    "        iou_threshold (float): threshold where predicted bboxes is correct\n",
    "        box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n",
    "        num_classes (int): number of classes\n",
    "\n",
    "    Returns:\n",
    "        float: mAP value across all classes given a specific IoU threshold \n",
    "    \"\"\"\n",
    "\n",
    "    # list storing all AP for respective classes\n",
    "    average_precisions = []\n",
    "\n",
    "    # used for numerical stability later on\n",
    "    epsilon = 1e-6\n",
    "\n",
    "    for c in range(num_classes):\n",
    "        detections = []\n",
    "        ground_truths = []\n",
    "\n",
    "        # Go through all predictions and targets,\n",
    "        # and only add the ones that belong to the\n",
    "        # current class c\n",
    "        for detection in pred_boxes:\n",
    "            if detection[1] == c:\n",
    "                detections.append(detection)\n",
    "\n",
    "        for true_box in true_boxes:\n",
    "            if true_box[1] == c:\n",
    "                ground_truths.append(true_box)\n",
    "\n",
    "        # find the amount of bboxes for each training example\n",
    "        # Counter here finds how many ground truth bboxes we get\n",
    "        # for each training example, so let's say img 0 has 3,\n",
    "        # img 1 has 5 then we will obtain a dictionary with:\n",
    "        # amount_bboxes = {0:3, 1:5}\n",
    "        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n",
    "\n",
    "        # We then go through each key, val in this dictionary\n",
    "        # and convert to the following (w.r.t same example):\n",
    "        # ammount_bboxes = {0:torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}\n",
    "        for key, val in amount_bboxes.items():\n",
    "            amount_bboxes[key] = torch.zeros(val)\n",
    "\n",
    "        # sort by box probabilities which is index 2\n",
    "        detections.sort(key=lambda x: x[2], reverse=True)\n",
    "        TP = torch.zeros((len(detections)))\n",
    "        FP = torch.zeros((len(detections)))\n",
    "        total_true_bboxes = len(ground_truths)\n",
    "        \n",
    "        # If none exists for this class then we can safely skip\n",
    "        if total_true_bboxes == 0:\n",
    "            continue\n",
    "\n",
    "        for detection_idx, detection in enumerate(detections):\n",
    "            # Only take out the ground_truths that have the same\n",
    "            # training idx as detection\n",
    "            ground_truth_img = [\n",
    "                bbox for bbox in ground_truths if bbox[0] == detection[0]\n",
    "            ]\n",
    "\n",
    "            num_gts = len(ground_truth_img)\n",
    "            best_iou = 0\n",
    "\n",
    "            for idx, gt in enumerate(ground_truth_img):\n",
    "                iou = intersection_over_union(\n",
    "                    torch.tensor(detection[3:]),\n",
    "                    torch.tensor(gt[3:]),\n",
    "                    box_format=box_format,\n",
    "                )\n",
    "\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_gt_idx = idx\n",
    "\n",
    "            if best_iou > iou_threshold:\n",
    "                # only detect ground truth detection once\n",
    "                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n",
    "                    # true positive and add this bounding box to seen\n",
    "                    TP[detection_idx] = 1\n",
    "                    amount_bboxes[detection[0]][best_gt_idx] = 1\n",
    "                else:\n",
    "                    FP[detection_idx] = 1\n",
    "\n",
    "            # if IOU is lower then the detection is a false positive\n",
    "            else:\n",
    "                FP[detection_idx] = 1\n",
    "\n",
    "        TP_cumsum = torch.cumsum(TP, dim=0)\n",
    "        FP_cumsum = torch.cumsum(FP, dim=0)\n",
    "        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n",
    "        precisions = torch.divide(TP_cumsum, (TP_cumsum + FP_cumsum + epsilon))\n",
    "        precisions = torch.cat((torch.tensor([1]), precisions))\n",
    "        recalls = torch.cat((torch.tensor([0]), recalls))\n",
    "        # torch.trapz for numerical integration\n",
    "        average_precisions.append(torch.trapz(precisions, recalls))\n",
    "\n",
    "    return sum(average_precisions) / len(average_precisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c329fd3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bce8f843",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(image, boxes):\n",
    "    \"\"\"Plots predicted bounding boxes on the image\"\"\"\n",
    "    im = np.array(image)\n",
    "    height, width, _ = im.shape\n",
    "\n",
    "    # Create figure and axes\n",
    "    fig, ax = plt.subplots(1)\n",
    "    # Display the image\n",
    "    ax.imshow(im)\n",
    "\n",
    "    # box[0] is x midpoint, box[2] is width\n",
    "    # box[1] is y midpoint, box[3] is height\n",
    "\n",
    "    # Create a Rectangle potch\n",
    "    for box in boxes:\n",
    "        box = box[2:]\n",
    "        assert len(box) == 4, \"Got more values than in x, y, w, h, in a box!\"\n",
    "        upper_left_x = box[0] - box[2] / 2\n",
    "        upper_left_y = box[1] - box[3] / 2\n",
    "        rect = patches.Rectangle(\n",
    "            (upper_left_x * width, upper_left_y * height),\n",
    "            box[2] * width,\n",
    "            box[3] * height,\n",
    "            linewidth=1,\n",
    "            edgecolor=\"r\",\n",
    "            facecolor=\"none\",\n",
    "        )\n",
    "        # Add the patch to the Axes\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80797326",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99e7ae40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bboxes(\n",
    "    loader,\n",
    "    model,\n",
    "    iou_threshold,\n",
    "    threshold,\n",
    "    \n",
    "    pred_format=\"cells\",\n",
    "    box_format=\"midpoint\",\n",
    "    device=\"cuda\",\n",
    "):\n",
    "    all_pred_boxes = []\n",
    "    all_true_boxes = []\n",
    "\n",
    "    # make sure model is in eval before get bboxes\n",
    "    model.eval()\n",
    "    train_idx = 0\n",
    "    \n",
    "    for batch_idx, (x, labels) in enumerate(loader):\n",
    "        x = x.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = model(x)\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        true_bboxes = cellboxes_to_boxes(labels)\n",
    "        bboxes = cellboxes_to_boxes(predictions)\n",
    "\n",
    "        for idx in range(batch_size):\n",
    "            nms_boxes = non_max_suppression(\n",
    "                bboxes[idx],\n",
    "                iou_threshold=iou_threshold,\n",
    "                threshold=threshold,\n",
    "                box_format=box_format,\n",
    "            )\n",
    "\n",
    "            #if batch_idx == 0 and idx == 0:\n",
    "            #    plot_image(x[idx].permute(1,2,0).to(\"cpu\"), nms_boxes)\n",
    "            #    print(nms_boxes)\n",
    "            for nms_box in nms_boxes:\n",
    "                all_pred_boxes.append([train_idx] + nms_box)\n",
    "                print('all_pred_boxes : ', all_pred_boxes)\n",
    "            for box in true_bboxes[idx]:\n",
    "                # many will get converted to 0 pred\n",
    "                if box[1] > threshold:\n",
    "                    all_true_boxes.append([train_idx] + box)\n",
    "                    \n",
    "            train_idx += 1\n",
    "\n",
    "    model.train()\n",
    "    return all_pred_boxes, all_true_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544f00da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "7441c905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_cellboxes(predictions, S=7):\n",
    "    \"\"\"\n",
    "    Converts bounding boxes output from Yolo with\n",
    "    an image split size of S into entire image ratios\n",
    "    rather than relative to cell ratios. Tried to do this\n",
    "    vectorized, but this resulted in quite difficult to read\n",
    "    code... Use as a black box? Or implement a more intuitive,\n",
    "    using 2 for loops iterating range(S) and convert them one\n",
    "    by one, resulting in a slower but more readable implementation.\n",
    "    \"\"\"\n",
    "    B=2\n",
    "    C=4\n",
    "    \n",
    "    predictions = predictions.to(\"cpu\")\n",
    "    batch_size = predictions.shape[0]\n",
    "    predictions = predictions.reshape(batch_size, 7, 7, 14)\n",
    "    \n",
    "    bboxes1 = predictions[..., (C+1):(C+5)]\n",
    "    bboxes2 = predictions[..., (C+6):(C+10)]\n",
    "    scores = torch.cat(\n",
    "        (predictions[..., C].unsqueeze(0), predictions[..., (C+5)].unsqueeze(0)), dim=0\n",
    "    )\n",
    "    \n",
    "    best_box = scores.argmax(0).unsqueeze(-1)\n",
    "    best_boxes = bboxes1 * (1 - best_box) + best_box * bboxes2\n",
    "    cell_indices = torch.arange(7).repeat(batch_size, 7, 1).unsqueeze(-1)\n",
    "    \n",
    "    x = 1 / S * (best_boxes[..., :1] + cell_indices)\n",
    "    y = 1 / S * (best_boxes[..., 1:2] + cell_indices.permute(0, 2, 1, 3))\n",
    "    w_y = 1 / S * best_boxes[..., 2:4]\n",
    "    \n",
    "    converted_bboxes = torch.cat((x, y, w_y), dim=-1)\n",
    "    predicted_class = predictions[..., :C].argmax(-1).unsqueeze(-1)\n",
    "    best_confidence = torch.max(predictions[..., C], predictions[..., (C+5)]).unsqueeze(\n",
    "        -1\n",
    "    )\n",
    "    converted_preds = torch.cat(\n",
    "        (predicted_class, best_confidence, converted_bboxes), dim=-1\n",
    "    )\n",
    "\n",
    "    return converted_preds\n",
    "\n",
    "\n",
    "def cellboxes_to_boxes(out, S=7):\n",
    "    converted_pred = convert_cellboxes(out).reshape(out.shape[0], S * S, -1)\n",
    "    converted_pred[..., 0] = converted_pred[..., 0].long()\n",
    "    all_bboxes = []\n",
    "    \n",
    "    for ex_idx in range(out.shape[0]):\n",
    "        bboxes = []\n",
    "\n",
    "        for bbox_idx in range(S * S):\n",
    "            bboxes.append([x.item() for x in converted_pred[ex_idx, bbox_idx, :]])\n",
    "        all_bboxes.append(bboxes)\n",
    "\n",
    "    return all_bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb01ed2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "277c0fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    torch.save(state, filename)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint, model, optimizer):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70c2b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
